{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTsEYdtov6tp"
      },
      "source": [
        "# Beijing air quality forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "nWkSHhqXrCqF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n",
            "TensorFlow version: 2.20.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For deep learning - we'll use TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, GRU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded successfully!\n",
            "Train shape: (30676, 12)\n",
            "Test shape: (13148, 11)\n"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "train = pd.read_csv('data/train.csv')\n",
        "test = pd.read_csv('data/test.csv')\n",
        "\n",
        "print(\"Data loaded successfully!\")\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRse3uqRrft5"
      },
      "source": [
        "# Data exploration\n",
        "\n",
        "Explore the dataset with statistics and visualizations to understand the data better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 30676 entries, 0 to 30675\n",
            "Data columns (total 12 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   No        30676 non-null  int64  \n",
            " 1   DEWP      30676 non-null  float64\n",
            " 2   TEMP      30676 non-null  float64\n",
            " 3   PRES      30676 non-null  float64\n",
            " 4   Iws       30676 non-null  float64\n",
            " 5   Is        30676 non-null  float64\n",
            " 6   Ir        30676 non-null  float64\n",
            " 7   datetime  30676 non-null  object \n",
            " 8   cbwd_NW   30676 non-null  float64\n",
            " 9   cbwd_SE   30676 non-null  float64\n",
            " 10  cbwd_cv   30676 non-null  float64\n",
            " 11  pm2.5     28755 non-null  float64\n",
            "dtypes: float64(10), int64(1), object(1)\n",
            "memory usage: 2.8+ MB\n",
            "None\n",
            "\n",
            "Train data describe:\n",
            "                 No          DEWP          TEMP          PRES           Iws  \\\n",
            "count  30676.000000  30676.000000  30676.000000  30676.000000  30676.000000   \n",
            "mean   15338.500000     -0.029431     -0.062712      0.013612      0.030542   \n",
            "std     8855.542765      0.994087      1.015193      1.008991      1.018337   \n",
            "min        1.000000     -2.135153     -2.578070     -2.380821     -0.468688   \n",
            "25%     7669.750000     -0.888034     -0.938521     -0.822670     -0.441894   \n",
            "50%    15338.500000     -0.056622      0.045209     -0.043595     -0.352512   \n",
            "75%    23007.250000      0.913358      0.864984      0.832865      0.005216   \n",
            "max    30676.000000      1.814055      2.340578      2.877939     11.231956   \n",
            "\n",
            "                 Is            Ir       cbwd_NW       cbwd_SE       cbwd_cv  \\\n",
            "count  30676.000000  30676.000000  30676.000000  30676.000000  30676.000000   \n",
            "mean       0.016992      0.011253      0.016193      0.005833     -0.025008   \n",
            "std        1.087278      1.063811      1.006001      1.001847      0.982122   \n",
            "min       -0.069353     -0.137667     -0.690542     -0.732019     -0.522096   \n",
            "25%       -0.069353     -0.137667     -0.690542     -0.732019     -0.522096   \n",
            "50%       -0.069353     -0.137667     -0.690542     -0.732019     -0.522096   \n",
            "75%       -0.069353     -0.137667      1.448138      1.366085     -0.522096   \n",
            "max       35.439859     25.288745      1.448138      1.366085      1.915355   \n",
            "\n",
            "              pm2.5  \n",
            "count  28755.000000  \n",
            "mean     100.793427  \n",
            "std       93.144433  \n",
            "min        0.000000  \n",
            "25%       29.000000  \n",
            "50%       75.000000  \n",
            "75%      142.000000  \n",
            "max      994.000000  \n",
            "\n",
            "Missing values:\n",
            "Train missing values:\n",
            "No             0\n",
            "DEWP           0\n",
            "TEMP           0\n",
            "PRES           0\n",
            "Iws            0\n",
            "Is             0\n",
            "Ir             0\n",
            "datetime       0\n",
            "cbwd_NW        0\n",
            "cbwd_SE        0\n",
            "cbwd_cv        0\n",
            "pm2.5       1921\n",
            "dtype: int64\n",
            "\n",
            "Test missing values:\n",
            "No          0\n",
            "DEWP        0\n",
            "TEMP        0\n",
            "PRES        0\n",
            "Iws         0\n",
            "Is          0\n",
            "Ir          0\n",
            "datetime    0\n",
            "cbwd_NW     0\n",
            "cbwd_SE     0\n",
            "cbwd_cv     0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Basic data exploration\n",
        "print(\"Train data info:\")\n",
        "print(train.info())\n",
        "print(\"\\nTrain data describe:\")\n",
        "print(train.describe())\n",
        "\n",
        "print(\"\\nMissing values:\")\n",
        "print(\"Train missing values:\")\n",
        "print(train.isnull().sum())\n",
        "print(\"\\nTest missing values:\")\n",
        "print(test.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "3R74CEBFrYok",
        "outputId": "0e593627-9c80-490c-826e-74e4df4a2249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Data Overview:\n",
            "          No      DEWP      TEMP      PRES       Iws        Is        Ir  \\\n",
            "30671  30672  1.467633  0.946961 -2.088668 -0.415099 -0.069353  2.687490   \n",
            "30672  30673  1.329064  0.864984 -2.186052 -0.379306 -0.069353  3.393779   \n",
            "30673  30674  1.259780  0.701029 -2.088668 -0.263130 -0.069353  4.100068   \n",
            "30674  30675  1.190496  0.701029 -2.088668 -0.146953 -0.069353  4.806358   \n",
            "30675  30676  1.190496  0.701029 -2.186052 -0.084366 -0.069353 -0.137667   \n",
            "\n",
            "                  datetime   cbwd_NW   cbwd_SE   cbwd_cv  pm2.5  \n",
            "30671  2013-07-01 23:00:00 -0.690542 -0.732019 -0.522096   50.0  \n",
            "30672  2013-07-02 00:00:00  1.448138 -0.732019 -0.522096   41.0  \n",
            "30673  2013-07-02 01:00:00  1.448138 -0.732019 -0.522096   32.0  \n",
            "30674  2013-07-02 02:00:00  1.448138 -0.732019 -0.522096   19.0  \n",
            "30675  2013-07-02 03:00:00  1.448138 -0.732019 -0.522096   18.0  \n",
            "First datetime: 2010-01-01 00:00:00\n",
            "Last datetime: 2013-07-02 03:00:00\n"
          ]
        }
      ],
      "source": [
        "# Inspecting the first few rows of the dataset to understand its structure.\n",
        "print(\"Training Data Overview:\")\n",
        "train.head()\n",
        "print(train.tail())\n",
        "print(f\"First datetime: {train['datetime'].iloc[0]}\")\n",
        "print(f\"Last datetime: {train['datetime'].iloc[-1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-om6hH_RtG8Z",
        "outputId": "8fefc873-d80f-4b45-ead2-89bbfc8d4d62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'datetime', 'cbwd_NW',\n",
              "       'cbwd_SE', 'cbwd_cv', 'pm2.5'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "35IGrMYRscQx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datetime preprocessing completed!\n",
            "Train date range: 2010-01-01 00:00:00 to 2013-07-02 03:00:00\n",
            "Test date range: 2013-07-02 04:00:00 to 2014-12-31 23:00:00\n"
          ]
        }
      ],
      "source": [
        "# Ensure 'datetime' column is in datetime format\n",
        "train['datetime'] = pd.to_datetime(train['datetime'])\n",
        "\n",
        "test['datetime'] = pd.to_datetime(test['datetime'])\n",
        "\n",
        "# Set the 'datetime' column as the index for better time-series handling\n",
        "train.set_index('datetime', inplace=True)\n",
        "# val.set_index('datetime', inplace=True)\n",
        "test.set_index('datetime', inplace=True)\n",
        "\n",
        "print(\"Datetime preprocessing completed!\")\n",
        "print(f\"Train date range: {train.index.min()} to {train.index.max()}\")\n",
        "print(f\"Test date range: {test.index.min()} to {test.index.max()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PM2.5 analysis\n",
            "PM2.5 min: 0.0\n",
            "PM2.5 max: 994.0\n",
            "PM2.5 mean: 100.79\n",
            "PM2.5 std: 93.14\n",
            "\n",
            "Missing PM2.5 pattern\n",
            "Missing values: 1921\n",
            "Percentage missing: 6.26%\n"
          ]
        }
      ],
      "source": [
        "# Examine PM2.5 values\n",
        "print(\"PM2.5 analysis\")\n",
        "print(f\"PM2.5 min: {train['pm2.5'].min()}\")\n",
        "print(f\"PM2.5 max: {train['pm2.5'].max()}\")\n",
        "print(f\"PM2.5 mean: {train['pm2.5'].mean():.2f}\")\n",
        "print(f\"PM2.5 std: {train['pm2.5'].std():.2f}\")\n",
        "\n",
        "# Check where missing values are located\n",
        "print(f\"\\nMissing PM2.5 pattern\")\n",
        "missing_count = train['pm2.5'].isnull().sum()\n",
        "print(f\"Missing values: {missing_count}\")\n",
        "print(f\"Percentage missing: {missing_count/len(train)*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detailed Missing PM2.5 Analysis\n",
            "First 20 rows of missing PM2.5:\n",
            "                     pm2.5\n",
            "datetime                  \n",
            "2010-01-01 00:00:00    NaN\n",
            "2010-01-01 01:00:00    NaN\n",
            "2010-01-01 02:00:00    NaN\n",
            "2010-01-01 03:00:00    NaN\n",
            "2010-01-01 04:00:00    NaN\n",
            "2010-01-01 05:00:00    NaN\n",
            "2010-01-01 06:00:00    NaN\n",
            "2010-01-01 07:00:00    NaN\n",
            "2010-01-01 08:00:00    NaN\n",
            "2010-01-01 09:00:00    NaN\n",
            "2010-01-01 10:00:00    NaN\n",
            "2010-01-01 11:00:00    NaN\n",
            "2010-01-01 12:00:00    NaN\n",
            "2010-01-01 13:00:00    NaN\n",
            "2010-01-01 14:00:00    NaN\n",
            "2010-01-01 15:00:00    NaN\n",
            "2010-01-01 16:00:00    NaN\n",
            "2010-01-01 17:00:00    NaN\n",
            "2010-01-01 18:00:00    NaN\n",
            "2010-01-01 19:00:00    NaN\n",
            "\n",
            "First valid PM2.5 measurement: 2010-01-02 00:00:00\n",
            "\n",
            "Missing values by month:\n",
            "month\n",
            "1     242\n",
            "2       8\n",
            "3     163\n",
            "4     192\n",
            "5     109\n",
            "6     178\n",
            "7      59\n",
            "8     363\n",
            "9     259\n",
            "10    134\n",
            "11     83\n",
            "12    131\n",
            "Name: pm2.5, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check the missing value pattern more thoroughly\n",
        "print(\"Detailed Missing PM2.5 Analysis\")\n",
        "print(\"First 20 rows of missing PM2.5:\")\n",
        "missing_mask = train['pm2.5'].isnull()\n",
        "print(train[missing_mask].head(20)[['pm2.5']])\n",
        "\n",
        "# Find where valid data starts\n",
        "first_valid_idx = train['pm2.5'].first_valid_index()\n",
        "print(f\"\\nFirst valid PM2.5 measurement: {first_valid_idx}\")\n",
        "\n",
        "# Check if missing values are scattered or concentrated\n",
        "print(f\"\\nMissing values by month:\")\n",
        "train_with_month = train.copy()\n",
        "train_with_month['month'] = train_with_month.index.month\n",
        "print(train_with_month.groupby('month')['pm2.5'].apply(lambda x: x.isnull().sum()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABAqt0Jztd5s"
      },
      "source": [
        "# Handle missing values\n",
        "\n",
        "Check the dataset for missing values and decide how to handle them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gap analysis\n",
            "Consecutive missing value periods:\n",
            "Number of missing periods: 163\n",
            "Average gap length: 11.8 hours\n",
            "Max gap length: 155 hours\n",
            "Min gap length: 1 hours\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Check the gaps more systematically\n",
        "print(\"Gap analysis\")\n",
        "# Find consecutive missing periods\n",
        "missing_periods = train['pm2.5'].isnull()\n",
        "# Group consecutive missing values\n",
        "groups = (missing_periods != missing_periods.shift()).cumsum()\n",
        "missing_groups = train[missing_periods].groupby(groups).size()\n",
        "\n",
        "print(\"Consecutive missing value periods:\")\n",
        "print(f\"Number of missing periods: {len(missing_groups)}\")\n",
        "print(f\"Average gap length: {missing_groups.mean():.1f} hours\")\n",
        "print(f\"Max gap length: {missing_groups.max()} hours\")\n",
        "print(f\"Min gap length: {missing_groups.min()} hours\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Handling Missing Values\n",
            "Original data points: 30676\n",
            "Missing PM2.5: 1921\n",
            "After dropping missing PM2.5: 28755\n",
            "Data retained: 93.7%\n",
            "Missing values after cleaning: 0\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Handle missing values\n",
        "print(\"Handling Missing Values\")\n",
        "print(f\"Original data points: {len(train)}\")\n",
        "print(f\"Missing PM2.5: {train['pm2.5'].isnull().sum()}\")\n",
        "\n",
        "# Simple dropna\n",
        "train_clean = train.dropna(subset=['pm2.5']).copy()\n",
        "print(f\"After dropping missing PM2.5: {len(train_clean)}\")\n",
        "print(f\"Data retained: {len(train_clean)/len(train)*100:.1f}%\")\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(f\"Missing values after cleaning: {train_clean.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time continuity check:\n",
            "Number of gaps > 1 hour: 162\n",
            "Largest gaps:\n",
            "datetime\n",
            "2010-09-27 16:00:00   6 days 12:00:00\n",
            "2012-12-28 13:00:00   5 days 08:00:00\n",
            "2011-10-07 16:00:00   4 days 04:00:00\n",
            "2011-03-21 16:00:00   3 days 20:00:00\n",
            "2010-09-30 21:00:00   3 days 05:00:00\n",
            "Name: datetime, dtype: timedelta64[ns]\n"
          ]
        }
      ],
      "source": [
        "# Check for time continuity (important for time series)\n",
        "print(\"Time continuity check:\")\n",
        "time_diff = train_clean.index.to_series().diff()\n",
        "expected_freq = pd.Timedelta(hours=1)\n",
        "\n",
        "# Find any gaps larger than 1 hour\n",
        "gaps = time_diff[time_diff > expected_freq]\n",
        "print(f\"Number of gaps > 1 hour: {len(gaps)}\")\n",
        "if len(gaps) > 0:\n",
        "    print(\"Largest gaps:\")\n",
        "    print(gaps.nlargest(5))\n",
        "else:\n",
        "    print(\"No gaps found - data is continuous!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Capping PM2.5 outliers above 429.0\n",
            "Creating optimized features...\n",
            "Optimized features: 45 (was 64)\n"
          ]
        }
      ],
      "source": [
        "# OPTIMIZED FEATURE ENGINEERING (Fewer but better features)\n",
        "def create_optimized_features(df, target_col='pm2.5', has_target=True):\n",
        "    \"\"\"Create only the most effective features\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Keep only the most predictive temporal features\n",
        "    df['hour'] = df.index.hour\n",
        "    df['day_of_week'] = df.index.dayofweek\n",
        "    df['month'] = df.index.month\n",
        "\n",
        "    # Cyclical encoding for key temporal patterns only\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
        "\n",
        "    # REDUCED lag features (only most predictive)\n",
        "    key_lags = [1, 2, 3, 6, 12]  # Remove 24-hour lag\n",
        "    key_features = ['DEWP', 'TEMP', 'PRES', 'Iws']\n",
        "\n",
        "    for lag in key_lags:\n",
        "        for col in key_features:\n",
        "            df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
        "\n",
        "    # Keep only MOST EFFECTIVE moving averages\n",
        "    for window in [3, 12]:  # Only 3h and 12h (remove 6h and 24h)\n",
        "        for col in key_features:\n",
        "            df[f'{col}_ma_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "    # Key interaction features only\n",
        "    df['temp_dewp_diff'] = df['TEMP'] - df['DEWP']\n",
        "    df['pressure_wind_interaction'] = df['PRES'] * df['Iws']\n",
        "\n",
        "    # Fill NaN and clean up\n",
        "    df = df.bfill().ffill()\n",
        "    df = df.drop(['hour', 'day_of_week', 'month'], axis=1, errors='ignore')\n",
        "\n",
        "    return df\n",
        "\n",
        "# Handle extreme outliers in PM2.5 (cap at 99th percentile)\n",
        "pm25_99th = np.percentile(train_clean['pm2.5'].dropna(), 99)\n",
        "print(f\"Capping PM2.5 outliers above {pm25_99th:.1f}\")\n",
        "\n",
        "train_capped = train_clean.copy()\n",
        "train_capped['pm2.5'] = np.clip(train_capped['pm2.5'], 0, pm25_99th)\n",
        "\n",
        "# Apply optimized feature engineering\n",
        "print(\"Creating optimized features...\")\n",
        "train_enhanced = create_optimized_features(train_capped, 'pm2.5', has_target=True)\n",
        "test_enhanced = create_optimized_features(test, 'pm2.5', has_target=False)\n",
        "\n",
        "print(f\"Optimized features: {len(train_enhanced.columns)} (was 64)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKndkdRuty1C"
      },
      "source": [
        "# Feature and target separation\n",
        "\n",
        "Separate features and target variable for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Features\n",
            "Columns in clean training data:\n",
            "['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'pm2.5']\n",
            "\n",
            "Data shape: (28755, 11)\n",
            "\n",
            "Feature Correlations with PM2.5\n",
            "pm2.5      1.000000\n",
            "Iws       -0.260250\n",
            "cbwd_NW   -0.231176\n",
            "DEWP       0.218187\n",
            "cbwd_cv    0.158033\n",
            "cbwd_SE    0.118986\n",
            "PRES      -0.107773\n",
            "Ir        -0.052288\n",
            "TEMP      -0.039601\n",
            "Is         0.022279\n",
            "No         0.017961\n",
            "Name: pm2.5, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Let's examine what features we have\n",
        "print(\"Current Features\")\n",
        "print(\"Columns in clean training data:\")\n",
        "print(train_clean.columns.tolist())\n",
        "print(f\"\\nData shape: {train_clean.shape}\")\n",
        "\n",
        "# Look at feature correlations with PM2.5\n",
        "print(\"\\nFeature Correlations with PM2.5\")\n",
        "correlations = train_clean.corr()['pm2.5'].sort_values(key=abs, ascending=False)\n",
        "print(correlations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "QETLRAo_tvQH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced Feature-target split\n",
            "Features in train only: 43\n",
            "Features in test only: 43\n",
            "Common features: 43\n",
            "Final feature count: 43\n",
            "X_train shape: (28755, 43)\n",
            "y_train shape: (28755,)\n",
            "X_test shape: (13148, 43)\n",
            "NaN in X_train: 0\n",
            "NaN in y_train: 0\n",
            "NaN in X_test: 0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Enhanced Feature-target split\")\n",
        "\n",
        "# Get feature columns that exist in BOTH train and test\n",
        "train_feature_cols = [col for col in train_enhanced.columns if col not in ['pm2.5', 'No']]\n",
        "test_feature_cols = [col for col in test_enhanced.columns if col not in ['No']]\n",
        "\n",
        "# Use only features that exist in both datasets\n",
        "common_features = list(set(train_feature_cols) & set(test_feature_cols))\n",
        "print(f\"Features in train only: {len(train_feature_cols)}\")\n",
        "print(f\"Features in test only: {len(test_feature_cols)}\")\n",
        "print(f\"Common features: {len(common_features)}\")\n",
        "\n",
        "# Features that exist in train but not test (PM2.5 lag features)\n",
        "train_only_features = list(set(train_feature_cols) - set(test_feature_cols))\n",
        "if train_only_features:\n",
        "    print(f\"Train-only features (will be excluded): {train_only_features}\")\n",
        "\n",
        "# Use common features for modeling\n",
        "feature_cols = common_features\n",
        "\n",
        "X_train_full = train_enhanced[feature_cols].values\n",
        "y_train_full = train_enhanced['pm2.5'].values\n",
        "X_test_full = test_enhanced[feature_cols].values\n",
        "\n",
        "print(f\"Final feature count: {len(feature_cols)}\")\n",
        "print(f\"X_train shape: {X_train_full.shape}\")\n",
        "print(f\"y_train shape: {y_train_full.shape}\")\n",
        "print(f\"X_test shape: {X_test_full.shape}\")\n",
        "\n",
        "# Check for any remaining NaN values\n",
        "print(f\"NaN in X_train: {np.isnan(X_train_full).sum()}\")\n",
        "print(f\"NaN in y_train: {np.isnan(y_train_full).sum()}\")\n",
        "print(f\"NaN in X_test: {np.isnan(X_test_full).sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced scaling with RobustScaler\n",
            "X_train_scaled shape: (28755, 43)\n",
            "X_test_scaled shape: (13148, 43)\n",
            "y_train_scaled shape: (28755,)\n",
            "Scaling completed with RobustScaler!\n"
          ]
        }
      ],
      "source": [
        "# IMPROVED SCALING (RobustScaler is better for time series with outliers)\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "print(\"Enhanced scaling with RobustScaler\")\n",
        "scaler_X = RobustScaler()  # Less sensitive to outliers than StandardScaler\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "# Fit and transform\n",
        "X_train_scaled = scaler_X.fit_transform(X_train_full)\n",
        "X_test_scaled = scaler_X.transform(X_test_full)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train_full.reshape(-1, 1)).flatten()\n",
        "\n",
        "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
        "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
        "print(f\"y_train_scaled shape: {y_train_scaled.shape}\")\n",
        "\n",
        "print(\"Scaling completed with RobustScaler!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequence length 24: 28731 samples\n",
            "Sequence length 36: 28719 samples\n",
            "Sequence length 48: 28707 samples\n",
            "Using sequence length: 36\n",
            "Optimized sequence shapes:\n",
            "  X_train_seq: (28719, 36, 43)\n",
            "  y_train_seq: (28719,)\n"
          ]
        }
      ],
      "source": [
        "# OPTIMIZED SEQUENCE LENGTH (Test to find best)\n",
        "def test_sequence_lengths(X_train_scaled, y_train_scaled):\n",
        "    \"\"\"Quick test to find optimal sequence length\"\"\"\n",
        "    lengths = [24, 36, 48]\n",
        "    results = {}\n",
        "\n",
        "    for seq_len in lengths:\n",
        "        X_seq, y_seq = create_sequences_improved(X_train_scaled, y_train_scaled, seq_len)\n",
        "        split_idx = int(len(X_seq) * 0.85)\n",
        "\n",
        "        # Quick validation\n",
        "        val_loss = np.var(y_seq[split_idx:])  # Simple baseline\n",
        "        results[seq_len] = len(X_seq)  # More data is often better\n",
        "        print(f\"Sequence length {seq_len}: {len(X_seq)} samples\")\n",
        "\n",
        "    # Choose length that gives most training data while being reasonable\n",
        "    best_length = 36  # Sweet spot between memory and data\n",
        "    print(f\"Using sequence length: {best_length}\")\n",
        "    return best_length\n",
        "\n",
        "SEQUENCE_LENGTH = test_sequence_lengths(X_train_scaled, y_train_scaled)\n",
        "\n",
        "# Create training sequences with optimized length\n",
        "X_train_seq, y_train_seq = create_sequences_improved(X_train_scaled, y_train_scaled, SEQUENCE_LENGTH)\n",
        "\n",
        "print(f\"Optimized sequence shapes:\")\n",
        "print(f\"  X_train_seq: {X_train_seq.shape}\")\n",
        "print(f\"  y_train_seq: {y_train_seq.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimized train-validation split\n",
            "Training samples: 24411 (increased)\n",
            "Validation samples: 4308 (decreased)\n",
            "More training data should improve performance\n"
          ]
        }
      ],
      "source": [
        "# OPTIMIZED TRAIN-VALIDATION SPLIT (More training data)\n",
        "print(\"Optimized train-validation split\")\n",
        "\n",
        "# Use 15% for validation (was 20%) = more training data\n",
        "val_size = 0.15\n",
        "split_idx = int(len(X_train_seq) * (1 - val_size))\n",
        "\n",
        "X_train_final = X_train_seq[:split_idx]\n",
        "X_val = X_train_seq[split_idx:]\n",
        "y_train_final = y_train_seq[:split_idx]\n",
        "y_val = y_train_seq[split_idx:]\n",
        "\n",
        "print(f\"Training samples: {len(X_train_final)} (increased)\")\n",
        "print(f\"Validation samples: {len(X_val)} (decreased)\")\n",
        "print(f\"More training data should improve performance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d488782wuR2W"
      },
      "source": [
        "# Model building\n",
        "\n",
        "Build and train LSTM model for time series forecasting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building optimized LSTM model...\n",
            "Optimized model architecture (simpler but effective):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">27,648</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m27,648\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,657</span> (119.75 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m30,657\u001b[0m (119.75 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,465</span> (119.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m30,465\u001b[0m (119.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# SIMPLIFIED BUT EFFECTIVE MODEL\n",
        "def create_optimized_model(input_shape):\n",
        "    \"\"\"Simpler model that often works better\"\"\"\n",
        "    model = Sequential([\n",
        "        # Single LSTM layer (simpler is better for time series)\n",
        "        LSTM(64, return_sequences=False, input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Small dense layers\n",
        "        Dense(32, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.15),\n",
        "\n",
        "        Dense(16, activation='relu'),\n",
        "        Dropout(0.1),\n",
        "\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Optimized learning rate\n",
        "    optimizer = Adam(\n",
        "        learning_rate=0.002,  # Slightly higher\n",
        "        clipnorm=1.0\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the optimized model\n",
        "print(\"Building optimized LSTM model...\")\n",
        "input_shape = (SEQUENCE_LENGTH, X_train_final.shape[2])\n",
        "model_optimized = create_optimized_model(input_shape)\n",
        "\n",
        "print(\"Optimized model architecture (simpler but effective):\")\n",
        "model_optimized.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimized training setup\n",
            "Training setup:\n",
            "  Epochs: 100\n",
            "  Batch size: 32\n",
            "  More aggressive learning rate schedule\n",
            "\n",
            "Starting optimized training...\n",
            "Epoch 1/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.5902 - mae: 0.5556 - val_loss: 0.7183 - val_mae: 0.5905 - learning_rate: 0.0020\n",
            "Epoch 2/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.4067 - mae: 0.4572 - val_loss: 0.6039 - val_mae: 0.5295 - learning_rate: 0.0020\n",
            "Epoch 3/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.3483 - mae: 0.4200 - val_loss: 0.6054 - val_mae: 0.5158 - learning_rate: 0.0020\n",
            "Epoch 4/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.3055 - mae: 0.3944 - val_loss: 0.6062 - val_mae: 0.5211 - learning_rate: 0.0020\n",
            "Epoch 5/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2727 - mae: 0.3725 - val_loss: 0.6443 - val_mae: 0.5370 - learning_rate: 0.0020\n",
            "Epoch 6/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2487 - mae: 0.3558 - val_loss: 0.6723 - val_mae: 0.5450 - learning_rate: 0.0020\n",
            "Epoch 7/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2284 - mae: 0.3417 - val_loss: 0.6339 - val_mae: 0.5464 - learning_rate: 0.0020\n",
            "Epoch 8/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.2168 - mae: 0.3327 - val_loss: 0.6403 - val_mae: 0.5374 - learning_rate: 0.0020\n",
            "Epoch 9/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.2036 - mae: 0.3235 - val_loss: 0.6078 - val_mae: 0.5215 - learning_rate: 0.0020\n",
            "Epoch 10/100\n",
            "\u001b[1m756/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2008 - mae: 0.3205\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.1926 - mae: 0.3148 - val_loss: 0.6547 - val_mae: 0.5532 - learning_rate: 0.0020\n",
            "Epoch 11/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.1727 - mae: 0.2983 - val_loss: 0.6403 - val_mae: 0.5254 - learning_rate: 0.0012\n",
            "Epoch 12/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.1630 - mae: 0.2916 - val_loss: 0.6735 - val_mae: 0.5339 - learning_rate: 0.0012\n",
            "Epoch 13/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.1585 - mae: 0.2866 - val_loss: 0.6502 - val_mae: 0.5329 - learning_rate: 0.0012\n",
            "Epoch 14/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.1536 - mae: 0.2834 - val_loss: 0.6911 - val_mae: 0.5527 - learning_rate: 0.0012\n",
            "Epoch 15/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.1492 - mae: 0.2796 - val_loss: 0.7029 - val_mae: 0.5506 - learning_rate: 0.0012\n",
            "Epoch 16/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.1465 - mae: 0.2781 - val_loss: 0.7160 - val_mae: 0.5478 - learning_rate: 0.0012\n",
            "Epoch 17/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.1464 - mae: 0.2761 - val_loss: 0.6914 - val_mae: 0.5485 - learning_rate: 0.0012\n",
            "Epoch 18/100\n",
            "\u001b[1m759/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1424 - mae: 0.2716\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.000720000034198165.\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.1377 - mae: 0.2697 - val_loss: 0.6764 - val_mae: 0.5486 - learning_rate: 0.0012\n",
            "Epoch 19/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.1275 - mae: 0.2600 - val_loss: 0.6661 - val_mae: 0.5421 - learning_rate: 7.2000e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.1280 - mae: 0.2597 - val_loss: 0.7026 - val_mae: 0.5499 - learning_rate: 7.2000e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.1282 - mae: 0.2597 - val_loss: 0.7096 - val_mae: 0.5567 - learning_rate: 7.2000e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m763/763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.1267 - mae: 0.2587 - val_loss: 0.7013 - val_mae: 0.5524 - learning_rate: 7.2000e-04\n",
            "Epoch 22: early stopping\n",
            "Restoring model weights from the end of the best epoch: 2.\n",
            "Optimized training completed!\n"
          ]
        }
      ],
      "source": [
        "# OPTIMIZED TRAINING SETUP\n",
        "print(\"Optimized training setup\")\n",
        "\n",
        "# More aggressive callbacks for better convergence\n",
        "callbacks_optimized = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=20,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.6,  # More aggressive LR reduction\n",
        "        patience=8,   # Faster LR reduction\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Training parameters\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "print(f\"Training setup:\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  More aggressive learning rate schedule\")\n",
        "\n",
        "print(\"\\nStarting optimized training...\")\n",
        "history_optimized = model_optimized.fit(\n",
        "    X_train_final, y_train_final,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks_optimized,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Optimized training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimized model evaluation\n",
            "OPTIMIZED MODEL PERFORMANCE:\n",
            "  Validation RMSE: 69.67 (Target: < 70)\n",
            "  Validation MAE: 47.47\n",
            "  Improvement from 72.37: 2.70 points\n",
            "  Target achieved: YES!\n",
            "\n",
            "IMPROVEMENT SUMMARY:\n",
            "  Previous RMSE: 72.37\n",
            "  Current RMSE: 69.67\n",
            "  Improvement: 2.70 points\n"
          ]
        }
      ],
      "source": [
        "# OPTIMIZED MODEL EVALUATION\n",
        "print(\"Optimized model evaluation\")\n",
        "\n",
        "# Make predictions on validation set\n",
        "val_pred_scaled = model_optimized.predict(X_val, verbose=0)\n",
        "val_pred_original = scaler_y.inverse_transform(val_pred_scaled.reshape(-1, 1)).flatten()\n",
        "val_true_original = scaler_y.inverse_transform(y_val.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "val_rmse_optimized = np.sqrt(np.mean((val_true_original - val_pred_original)**2))\n",
        "val_mae_optimized = np.mean(np.abs(val_true_original - val_pred_original))\n",
        "\n",
        "print(f\"OPTIMIZED MODEL PERFORMANCE:\")\n",
        "print(f\"  Validation RMSE: {val_rmse_optimized:.2f} (Target: < 70)\")\n",
        "print(f\"  Validation MAE: {val_mae_optimized:.2f}\")\n",
        "print(f\"  Improvement from 72.37: {72.37 - val_rmse_optimized:.2f} points\")\n",
        "print(f\"  Target achieved: {'YES!' if val_rmse_optimized < 70 else 'Close!'}\")\n",
        "\n",
        "# Show improvement\n",
        "improvement = 72.37 - val_rmse_optimized\n",
        "print(f\"\\nIMPROVEMENT SUMMARY:\")\n",
        "print(f\"  Previous RMSE: 72.37\")\n",
        "print(f\"  Current RMSE: {val_rmse_optimized:.2f}\")\n",
        "print(f\"  Improvement: {improvement:.2f} points\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating enhanced test predictions\n",
            "Last training samples: (36, 43)\n",
            "Combined data shape: (13184, 43)\n",
            "Test sequences shape: (13148, 36, 43)\n",
            "Making improved predictions...\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Test prediction statistics:\n",
            "  Min: 4.6\n",
            "  Max: 396.0\n",
            "  Mean: 92.6\n",
            "Submissions saved as:\n",
            "  Timestamped: submission_RMSE_69_67_20250921_150622.csv\n",
            "  Versioned: submission_RMSE_69_67_v2.csv\n",
            "Validation RMSE: 69.67\n"
          ]
        }
      ],
      "source": [
        "# IMPROVED TEST PREDICTIONS WITH DYNAMIC NAMING\n",
        "import datetime\n",
        "\n",
        "print(\"Creating enhanced test predictions\")\n",
        "\n",
        "# Create test sequences with improved method\n",
        "last_train_X = X_train_scaled[-SEQUENCE_LENGTH:]\n",
        "combined_X = np.vstack([last_train_X, X_test_scaled])\n",
        "\n",
        "print(f\"Last training samples: {last_train_X.shape}\")\n",
        "print(f\"Combined data shape: {combined_X.shape}\")\n",
        "\n",
        "# Create test sequences\n",
        "X_test_sequences = []\n",
        "for i in range(SEQUENCE_LENGTH, len(combined_X)):\n",
        "    X_test_sequences.append(combined_X[i-SEQUENCE_LENGTH:i])\n",
        "\n",
        "X_test_sequences = np.array(X_test_sequences)\n",
        "print(f\"Test sequences shape: {X_test_sequences.shape}\")\n",
        "\n",
        "# Make predictions\n",
        "print(\"Making improved predictions...\")\n",
        "test_pred_scaled = model_optimized.predict(X_test_sequences, verbose=1)\n",
        "test_pred_original = scaler_y.inverse_transform(test_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Ensure no negative predictions\n",
        "test_pred_original = np.maximum(test_pred_original, 0)\n",
        "\n",
        "print(f\"Test prediction statistics:\")\n",
        "print(f\"  Min: {test_pred_original.min():.1f}\")\n",
        "print(f\"  Max: {test_pred_original.max():.1f}\")\n",
        "print(f\"  Mean: {test_pred_original.mean():.1f}\")\n",
        "\n",
        "# Create submission\n",
        "def format_datetime_no_leading_zero(dt):\n",
        "    return f\"{dt.year}-{dt.month:02d}-{dt.day:02d} {dt.hour}:{dt.minute:02d}:{dt.second:02d}\"\n",
        "\n",
        "row_ids_formatted = [format_datetime_no_leading_zero(dt) for dt in test.index]\n",
        "\n",
        "submission_optimized = pd.DataFrame({\n",
        "    'row ID': row_ids_formatted,\n",
        "    'pm2.5': test_pred_original.astype(int)\n",
        "})\n",
        "\n",
        "# Generate dynamic filename with timestamp and RMSE\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "rmse_str = f\"{val_rmse_optimized:.2f}\".replace(\".\", \"_\")\n",
        "submission_path = f'submission_RMSE_{rmse_str}_{timestamp}.csv'\n",
        "\n",
        "# Alternative: Simple counter-based naming\n",
        "import os\n",
        "counter = 1\n",
        "base_name = f'submission_RMSE_{rmse_str}'\n",
        "while os.path.exists(f'{base_name}_v{counter}.csv'):\n",
        "    counter += 1\n",
        "submission_path_alt = f'{base_name}_v{counter}.csv'\n",
        "\n",
        "# Save both versions\n",
        "submission_optimized.to_csv(submission_path, index=False)\n",
        "submission_optimized.to_csv(submission_path_alt, index=False)\n",
        "\n",
        "print(f\"Submissions saved as:\")\n",
        "print(f\"  Timestamped: {submission_path}\")\n",
        "print(f\"  Versioned: {submission_path_alt}\")\n",
        "print(f\"Validation RMSE: {val_rmse_optimized:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
