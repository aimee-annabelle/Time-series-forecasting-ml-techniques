{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTsEYdtov6tp"
      },
      "source": [
        "# Beijing air quality forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nWkSHhqXrCqF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n",
            "TensorFlow version: 2.20.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For deep learning - we'll use TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, GRU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded successfully!\n",
            "Train shape: (30676, 12)\n",
            "Test shape: (13148, 11)\n"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "train = pd.read_csv('data/train.csv')\n",
        "test = pd.read_csv('data/test.csv')\n",
        "\n",
        "print(\"Data loaded successfully!\")\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRse3uqRrft5"
      },
      "source": [
        "# Data exploration\n",
        "\n",
        "Explore the dataset with statistics and visualizations to understand the data better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 30676 entries, 0 to 30675\n",
            "Data columns (total 12 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   No        30676 non-null  int64  \n",
            " 1   DEWP      30676 non-null  float64\n",
            " 2   TEMP      30676 non-null  float64\n",
            " 3   PRES      30676 non-null  float64\n",
            " 4   Iws       30676 non-null  float64\n",
            " 5   Is        30676 non-null  float64\n",
            " 6   Ir        30676 non-null  float64\n",
            " 7   datetime  30676 non-null  object \n",
            " 8   cbwd_NW   30676 non-null  float64\n",
            " 9   cbwd_SE   30676 non-null  float64\n",
            " 10  cbwd_cv   30676 non-null  float64\n",
            " 11  pm2.5     28755 non-null  float64\n",
            "dtypes: float64(10), int64(1), object(1)\n",
            "memory usage: 2.8+ MB\n",
            "None\n",
            "\n",
            "Train data describe:\n",
            "                 No          DEWP          TEMP          PRES           Iws  \\\n",
            "count  30676.000000  30676.000000  30676.000000  30676.000000  30676.000000   \n",
            "mean   15338.500000     -0.029431     -0.062712      0.013612      0.030542   \n",
            "std     8855.542765      0.994087      1.015193      1.008991      1.018337   \n",
            "min        1.000000     -2.135153     -2.578070     -2.380821     -0.468688   \n",
            "25%     7669.750000     -0.888034     -0.938521     -0.822670     -0.441894   \n",
            "50%    15338.500000     -0.056622      0.045209     -0.043595     -0.352512   \n",
            "75%    23007.250000      0.913358      0.864984      0.832865      0.005216   \n",
            "max    30676.000000      1.814055      2.340578      2.877939     11.231956   \n",
            "\n",
            "                 Is            Ir       cbwd_NW       cbwd_SE       cbwd_cv  \\\n",
            "count  30676.000000  30676.000000  30676.000000  30676.000000  30676.000000   \n",
            "mean       0.016992      0.011253      0.016193      0.005833     -0.025008   \n",
            "std        1.087278      1.063811      1.006001      1.001847      0.982122   \n",
            "min       -0.069353     -0.137667     -0.690542     -0.732019     -0.522096   \n",
            "25%       -0.069353     -0.137667     -0.690542     -0.732019     -0.522096   \n",
            "50%       -0.069353     -0.137667     -0.690542     -0.732019     -0.522096   \n",
            "75%       -0.069353     -0.137667      1.448138      1.366085     -0.522096   \n",
            "max       35.439859     25.288745      1.448138      1.366085      1.915355   \n",
            "\n",
            "              pm2.5  \n",
            "count  28755.000000  \n",
            "mean     100.793427  \n",
            "std       93.144433  \n",
            "min        0.000000  \n",
            "25%       29.000000  \n",
            "50%       75.000000  \n",
            "75%      142.000000  \n",
            "max      994.000000  \n",
            "\n",
            "Missing values:\n",
            "Train missing values:\n",
            "No             0\n",
            "DEWP           0\n",
            "TEMP           0\n",
            "PRES           0\n",
            "Iws            0\n",
            "Is             0\n",
            "Ir             0\n",
            "datetime       0\n",
            "cbwd_NW        0\n",
            "cbwd_SE        0\n",
            "cbwd_cv        0\n",
            "pm2.5       1921\n",
            "dtype: int64\n",
            "\n",
            "Test missing values:\n",
            "No          0\n",
            "DEWP        0\n",
            "TEMP        0\n",
            "PRES        0\n",
            "Iws         0\n",
            "Is          0\n",
            "Ir          0\n",
            "datetime    0\n",
            "cbwd_NW     0\n",
            "cbwd_SE     0\n",
            "cbwd_cv     0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Basic data exploration\n",
        "print(\"Train data info:\")\n",
        "print(train.info())\n",
        "print(\"\\nTrain data describe:\")\n",
        "print(train.describe())\n",
        "\n",
        "print(\"\\nMissing values:\")\n",
        "print(\"Train missing values:\")\n",
        "print(train.isnull().sum())\n",
        "print(\"\\nTest missing values:\")\n",
        "print(test.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "3R74CEBFrYok",
        "outputId": "0e593627-9c80-490c-826e-74e4df4a2249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Data Overview:\n",
            "          No      DEWP      TEMP      PRES       Iws        Is        Ir  \\\n",
            "30671  30672  1.467633  0.946961 -2.088668 -0.415099 -0.069353  2.687490   \n",
            "30672  30673  1.329064  0.864984 -2.186052 -0.379306 -0.069353  3.393779   \n",
            "30673  30674  1.259780  0.701029 -2.088668 -0.263130 -0.069353  4.100068   \n",
            "30674  30675  1.190496  0.701029 -2.088668 -0.146953 -0.069353  4.806358   \n",
            "30675  30676  1.190496  0.701029 -2.186052 -0.084366 -0.069353 -0.137667   \n",
            "\n",
            "                  datetime   cbwd_NW   cbwd_SE   cbwd_cv  pm2.5  \n",
            "30671  2013-07-01 23:00:00 -0.690542 -0.732019 -0.522096   50.0  \n",
            "30672  2013-07-02 00:00:00  1.448138 -0.732019 -0.522096   41.0  \n",
            "30673  2013-07-02 01:00:00  1.448138 -0.732019 -0.522096   32.0  \n",
            "30674  2013-07-02 02:00:00  1.448138 -0.732019 -0.522096   19.0  \n",
            "30675  2013-07-02 03:00:00  1.448138 -0.732019 -0.522096   18.0  \n",
            "First datetime: 2010-01-01 00:00:00\n",
            "Last datetime: 2013-07-02 03:00:00\n"
          ]
        }
      ],
      "source": [
        "# Inspecting the first few rows of the dataset to understand its structure.\n",
        "print(\"Training Data Overview:\")\n",
        "train.head()\n",
        "print(train.tail())\n",
        "print(f\"First datetime: {train['datetime'].iloc[0]}\")\n",
        "print(f\"Last datetime: {train['datetime'].iloc[-1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-om6hH_RtG8Z",
        "outputId": "8fefc873-d80f-4b45-ead2-89bbfc8d4d62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'datetime', 'cbwd_NW',\n",
              "       'cbwd_SE', 'cbwd_cv', 'pm2.5'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "35IGrMYRscQx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datetime preprocessing completed!\n",
            "Train date range: 2010-01-01 00:00:00 to 2013-07-02 03:00:00\n",
            "Test date range: 2013-07-02 04:00:00 to 2014-12-31 23:00:00\n"
          ]
        }
      ],
      "source": [
        "# Ensure 'datetime' column is in datetime format\n",
        "train['datetime'] = pd.to_datetime(train['datetime'])\n",
        "\n",
        "test['datetime'] = pd.to_datetime(test['datetime'])\n",
        "\n",
        "# Set the 'datetime' column as the index for better time-series handling\n",
        "train.set_index('datetime', inplace=True)\n",
        "# val.set_index('datetime', inplace=True)\n",
        "test.set_index('datetime', inplace=True)\n",
        "\n",
        "print(\"Datetime preprocessing completed!\")\n",
        "print(f\"Train date range: {train.index.min()} to {train.index.max()}\")\n",
        "print(f\"Test date range: {test.index.min()} to {test.index.max()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PM2.5 analysis\n",
            "PM2.5 min: 0.0\n",
            "PM2.5 max: 994.0\n",
            "PM2.5 mean: 100.79\n",
            "PM2.5 std: 93.14\n",
            "\n",
            "Missing PM2.5 pattern\n",
            "Missing values: 1921\n",
            "Percentage missing: 6.26%\n"
          ]
        }
      ],
      "source": [
        "# Examine PM2.5 values\n",
        "print(\"PM2.5 analysis\")\n",
        "print(f\"PM2.5 min: {train['pm2.5'].min()}\")\n",
        "print(f\"PM2.5 max: {train['pm2.5'].max()}\")\n",
        "print(f\"PM2.5 mean: {train['pm2.5'].mean():.2f}\")\n",
        "print(f\"PM2.5 std: {train['pm2.5'].std():.2f}\")\n",
        "\n",
        "# Check where missing values are located\n",
        "print(f\"\\nMissing PM2.5 pattern\")\n",
        "missing_count = train['pm2.5'].isnull().sum()\n",
        "print(f\"Missing values: {missing_count}\")\n",
        "print(f\"Percentage missing: {missing_count/len(train)*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detailed Missing PM2.5 Analysis\n",
            "First 20 rows of missing PM2.5:\n",
            "                     pm2.5\n",
            "datetime                  \n",
            "2010-01-01 00:00:00    NaN\n",
            "2010-01-01 01:00:00    NaN\n",
            "2010-01-01 02:00:00    NaN\n",
            "2010-01-01 03:00:00    NaN\n",
            "2010-01-01 04:00:00    NaN\n",
            "2010-01-01 05:00:00    NaN\n",
            "2010-01-01 06:00:00    NaN\n",
            "2010-01-01 07:00:00    NaN\n",
            "2010-01-01 08:00:00    NaN\n",
            "2010-01-01 09:00:00    NaN\n",
            "2010-01-01 10:00:00    NaN\n",
            "2010-01-01 11:00:00    NaN\n",
            "2010-01-01 12:00:00    NaN\n",
            "2010-01-01 13:00:00    NaN\n",
            "2010-01-01 14:00:00    NaN\n",
            "2010-01-01 15:00:00    NaN\n",
            "2010-01-01 16:00:00    NaN\n",
            "2010-01-01 17:00:00    NaN\n",
            "2010-01-01 18:00:00    NaN\n",
            "2010-01-01 19:00:00    NaN\n",
            "\n",
            "First valid PM2.5 measurement: 2010-01-02 00:00:00\n",
            "\n",
            "Missing values by month:\n",
            "month\n",
            "1     242\n",
            "2       8\n",
            "3     163\n",
            "4     192\n",
            "5     109\n",
            "6     178\n",
            "7      59\n",
            "8     363\n",
            "9     259\n",
            "10    134\n",
            "11     83\n",
            "12    131\n",
            "Name: pm2.5, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check the missing value pattern more thoroughly\n",
        "print(\"Detailed Missing PM2.5 Analysis\")\n",
        "print(\"First 20 rows of missing PM2.5:\")\n",
        "missing_mask = train['pm2.5'].isnull()\n",
        "print(train[missing_mask].head(20)[['pm2.5']])\n",
        "\n",
        "# Find where valid data starts\n",
        "first_valid_idx = train['pm2.5'].first_valid_index()\n",
        "print(f\"\\nFirst valid PM2.5 measurement: {first_valid_idx}\")\n",
        "\n",
        "# Check if missing values are scattered or concentrated\n",
        "print(f\"\\nMissing values by month:\")\n",
        "train_with_month = train.copy()\n",
        "train_with_month['month'] = train_with_month.index.month\n",
        "print(train_with_month.groupby('month')['pm2.5'].apply(lambda x: x.isnull().sum()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABAqt0Jztd5s"
      },
      "source": [
        "# Handle missing values\n",
        "\n",
        "Check the dataset for missing values and decide how to handle them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gap analysis\n",
            "Consecutive missing value periods:\n",
            "Number of missing periods: 163\n",
            "Average gap length: 11.8 hours\n",
            "Max gap length: 155 hours\n",
            "Min gap length: 1 hours\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Check the gaps more systematically\n",
        "print(\"Gap analysis\")\n",
        "# Find consecutive missing periods\n",
        "missing_periods = train['pm2.5'].isnull()\n",
        "# Group consecutive missing values\n",
        "groups = (missing_periods != missing_periods.shift()).cumsum()\n",
        "missing_groups = train[missing_periods].groupby(groups).size()\n",
        "\n",
        "print(\"Consecutive missing value periods:\")\n",
        "print(f\"Number of missing periods: {len(missing_groups)}\")\n",
        "print(f\"Average gap length: {missing_groups.mean():.1f} hours\")\n",
        "print(f\"Max gap length: {missing_groups.max()} hours\")\n",
        "print(f\"Min gap length: {missing_groups.min()} hours\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Handling Missing Values\n",
            "Original data points: 30676\n",
            "Missing PM2.5: 1921\n",
            "After dropping missing PM2.5: 28755\n",
            "Data retained: 93.7%\n",
            "Missing values after cleaning: 0\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Handle missing values\n",
        "print(\"Handling Missing Values\")\n",
        "print(f\"Original data points: {len(train)}\")\n",
        "print(f\"Missing PM2.5: {train['pm2.5'].isnull().sum()}\")\n",
        "\n",
        "# Simple dropna\n",
        "train_clean = train.dropna(subset=['pm2.5']).copy()\n",
        "print(f\"After dropping missing PM2.5: {len(train_clean)}\")\n",
        "print(f\"Data retained: {len(train_clean)/len(train)*100:.1f}%\")\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(f\"Missing values after cleaning: {train_clean.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time continuity check:\n",
            "Number of gaps > 1 hour: 162\n",
            "Largest gaps:\n",
            "datetime\n",
            "2010-09-27 16:00:00   6 days 12:00:00\n",
            "2012-12-28 13:00:00   5 days 08:00:00\n",
            "2011-10-07 16:00:00   4 days 04:00:00\n",
            "2011-03-21 16:00:00   3 days 20:00:00\n",
            "2010-09-30 21:00:00   3 days 05:00:00\n",
            "Name: datetime, dtype: timedelta64[ns]\n"
          ]
        }
      ],
      "source": [
        "# Check for time continuity (important for time series)\n",
        "print(\"Time continuity check:\")\n",
        "time_diff = train_clean.index.to_series().diff()\n",
        "expected_freq = pd.Timedelta(hours=1)\n",
        "\n",
        "# Find any gaps larger than 1 hour\n",
        "gaps = time_diff[time_diff > expected_freq]\n",
        "print(f\"Number of gaps > 1 hour: {len(gaps)}\")\n",
        "if len(gaps) > 0:\n",
        "    print(\"Largest gaps:\")\n",
        "    print(gaps.nlargest(5))\n",
        "else:\n",
        "    print(\"No gaps found - data is continuous!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Advanced outlier capping at 368.0\n",
            "Final optimized features: 68\n"
          ]
        }
      ],
      "source": [
        "# FINAL OPTIMIZED FEATURE ENGINEERING\n",
        "def create_final_optimized_features(df, target_col='pm2.5', has_target=True):\n",
        "    \"\"\"Create the most predictive features for final optimization\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Enhanced temporal features\n",
        "    df['hour'] = df.index.hour\n",
        "    df['day_of_week'] = df.index.dayofweek\n",
        "    df['month'] = df.index.month\n",
        "    df['day_of_year'] = df.index.dayofyear\n",
        "\n",
        "    # More sophisticated cyclical encoding\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "\n",
        "    # Add pollution-specific patterns\n",
        "    df['is_winter'] = ((df['month'] >= 11) | (df['month'] <= 2)).astype(int)\n",
        "    df['is_heating_season'] = ((df['month'] >= 11) | (df['month'] <= 3)).astype(int)\n",
        "    df['is_rush_hour'] = ((df['hour'] >= 7) & (df['hour'] <= 9) |\n",
        "                         (df['hour'] >= 17) & (df['hour'] <= 19)).astype(int)\n",
        "\n",
        "    # Critical lag features\n",
        "    critical_lags = [1, 2, 3, 6, 12]\n",
        "    key_features = ['DEWP', 'TEMP', 'PRES', 'Iws']\n",
        "\n",
        "    for lag in critical_lags:\n",
        "        for col in key_features:\n",
        "            df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
        "\n",
        "    # Enhanced moving averages with volatility\n",
        "    for window in [3, 6, 12]:\n",
        "        for col in key_features:\n",
        "            df[f'{col}_ma_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n",
        "            df[f'{col}_std_{window}'] = df[col].rolling(window=window, min_periods=1).std()\n",
        "\n",
        "    # Advanced interaction features\n",
        "    df['temp_dewp_diff'] = df['TEMP'] - df['DEWP']\n",
        "    df['pressure_wind_interaction'] = df['PRES'] * df['Iws']\n",
        "    df['humidity_proxy'] = df['DEWP'] / (df['TEMP'] + 1e-8)\n",
        "    df['wind_stability'] = 1 / (df[['cbwd_NW', 'cbwd_SE', 'cbwd_cv']].std(axis=1) + 1e-8)\n",
        "\n",
        "    # Fill NaN and clean up\n",
        "    df = df.bfill().ffill()\n",
        "    df = df.drop(['hour', 'day_of_week', 'month', 'day_of_year'], axis=1, errors='ignore')\n",
        "\n",
        "    return df\n",
        "\n",
        "# Advanced outlier treatment\n",
        "Q1 = train_clean['pm2.5'].quantile(0.25)\n",
        "Q3 = train_clean['pm2.5'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "upper_bound = Q3 + 2 * IQR\n",
        "\n",
        "print(f\"Advanced outlier capping at {upper_bound:.1f}\")\n",
        "train_treated = train_clean.copy()\n",
        "train_treated['pm2.5'] = np.clip(train_treated['pm2.5'], 0, upper_bound)\n",
        "\n",
        "# Apply final feature engineering\n",
        "train_enhanced = create_final_optimized_features(train_treated, 'pm2.5', has_target=True)\n",
        "test_enhanced = create_final_optimized_features(test, 'pm2.5', has_target=False)\n",
        "\n",
        "print(f\"Final optimized features: {len(train_enhanced.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKndkdRuty1C"
      },
      "source": [
        "# Feature and target separation\n",
        "\n",
        "Separate features and target variable for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced Features Analysis\n",
            "Columns in enhanced training data:\n",
            "['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'pm2.5', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'is_winter', 'is_heating_season', 'is_rush_hour', 'DEWP_lag_1', 'TEMP_lag_1', 'PRES_lag_1', 'Iws_lag_1', 'DEWP_lag_2', 'TEMP_lag_2', 'PRES_lag_2', 'Iws_lag_2', 'DEWP_lag_3', 'TEMP_lag_3', 'PRES_lag_3', 'Iws_lag_3', 'DEWP_lag_6', 'TEMP_lag_6', 'PRES_lag_6', 'Iws_lag_6', 'DEWP_lag_12', 'TEMP_lag_12', 'PRES_lag_12', 'Iws_lag_12', 'DEWP_ma_3', 'DEWP_std_3', 'TEMP_ma_3', 'TEMP_std_3', 'PRES_ma_3', 'PRES_std_3', 'Iws_ma_3', 'Iws_std_3', 'DEWP_ma_6', 'DEWP_std_6', 'TEMP_ma_6', 'TEMP_std_6', 'PRES_ma_6', 'PRES_std_6', 'Iws_ma_6', 'Iws_std_6', 'DEWP_ma_12', 'DEWP_std_12', 'TEMP_ma_12', 'TEMP_std_12', 'PRES_ma_12', 'PRES_std_12', 'Iws_ma_12', 'Iws_std_12', 'temp_dewp_diff', 'pressure_wind_interaction', 'humidity_proxy', 'wind_stability']\n",
            "\n",
            "Enhanced data shape: (28755, 68)\n",
            "Original shape was: (28755, 11)\n",
            "Feature increase: +57 features\n",
            "\n",
            "Top Feature Correlations with PM2.5 (Enhanced)\n",
            "Top 15 most correlated features:\n",
            "pm2.5             1.000000\n",
            "temp_dewp_diff   -0.451493\n",
            "Iws_ma_6         -0.279490\n",
            "Iws_ma_12        -0.278743\n",
            "Iws_std_12       -0.278108\n",
            "Iws_ma_3         -0.277387\n",
            "Iws              -0.272003\n",
            "Iws_lag_1        -0.271470\n",
            "Iws_lag_2        -0.268852\n",
            "Iws_lag_3        -0.264345\n",
            "DEWP_std_12      -0.254657\n",
            "cbwd_NW          -0.247287\n",
            "Iws_lag_6        -0.247210\n",
            "DEWP              0.240594\n",
            "DEWP_lag_1        0.237832\n",
            "Name: pm2.5, dtype: float64\n",
            "\n",
            "New feature categories added:\n",
            "- Temporal features: 8\n",
            "- Lag features: 20\n",
            "- Moving averages: 12\n",
            "- Volatility features: 12\n",
            "- Interaction features: 4\n",
            "\n",
            "Total new features: 57\n"
          ]
        }
      ],
      "source": [
        "# UPDATED FEATURE ANALYSIS FOR ENHANCED FEATURES\n",
        "print(\"Enhanced Features Analysis\")\n",
        "print(\"Columns in enhanced training data:\")\n",
        "print(train_enhanced.columns.tolist())\n",
        "print(f\"\\nEnhanced data shape: {train_enhanced.shape}\")\n",
        "print(f\"Original shape was: {train_clean.shape}\")\n",
        "print(f\"Feature increase: +{len(train_enhanced.columns) - len(train_clean.columns)} features\")\n",
        "\n",
        "# Look at feature correlations with PM2.5 for enhanced features\n",
        "print(\"\\nTop Feature Correlations with PM2.5 (Enhanced)\")\n",
        "correlations = train_enhanced.corr()['pm2.5'].sort_values(key=abs, ascending=False)\n",
        "print(\"Top 15 most correlated features:\")\n",
        "print(correlations.head(15))\n",
        "\n",
        "# Show new feature types\n",
        "print(f\"\\nNew feature categories added:\")\n",
        "new_features = [col for col in train_enhanced.columns if col not in train_clean.columns]\n",
        "print(f\"- Temporal features: {len([f for f in new_features if any(x in f for x in ['sin', 'cos', 'hour', 'winter', 'rush'])])}\")\n",
        "print(f\"- Lag features: {len([f for f in new_features if 'lag' in f])}\")\n",
        "print(f\"- Moving averages: {len([f for f in new_features if 'ma_' in f])}\")\n",
        "print(f\"- Volatility features: {len([f for f in new_features if 'std_' in f])}\")\n",
        "print(f\"- Interaction features: {len([f for f in new_features if any(x in f for x in ['diff', 'interaction', 'proxy', 'stability'])])}\")\n",
        "\n",
        "print(f\"\\nTotal new features: {len(new_features)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QETLRAo_tvQH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced Feature-target split\n",
            "Features in train only: 66\n",
            "Features in test only: 66\n",
            "Common features: 66\n",
            "Final feature count: 66\n",
            "X_train shape: (28755, 66)\n",
            "y_train shape: (28755,)\n",
            "X_test shape: (13148, 66)\n",
            "NaN in X_train: 0\n",
            "NaN in y_train: 0\n",
            "NaN in X_test: 0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Enhanced Feature-target split\")\n",
        "\n",
        "# Get feature columns that exist in BOTH train and test\n",
        "train_feature_cols = [col for col in train_enhanced.columns if col not in ['pm2.5', 'No']]\n",
        "test_feature_cols = [col for col in test_enhanced.columns if col not in ['No']]\n",
        "\n",
        "# Use only features that exist in both datasets\n",
        "common_features = list(set(train_feature_cols) & set(test_feature_cols))\n",
        "print(f\"Features in train only: {len(train_feature_cols)}\")\n",
        "print(f\"Features in test only: {len(test_feature_cols)}\")\n",
        "print(f\"Common features: {len(common_features)}\")\n",
        "\n",
        "# Features that exist in train but not test (PM2.5 lag features)\n",
        "train_only_features = list(set(train_feature_cols) - set(test_feature_cols))\n",
        "if train_only_features:\n",
        "    print(f\"Train-only features (will be excluded): {train_only_features}\")\n",
        "\n",
        "# Use common features for modeling\n",
        "feature_cols = common_features\n",
        "\n",
        "X_train_full = train_enhanced[feature_cols].values\n",
        "y_train_full = train_enhanced['pm2.5'].values\n",
        "X_test_full = test_enhanced[feature_cols].values\n",
        "\n",
        "print(f\"Final feature count: {len(feature_cols)}\")\n",
        "print(f\"X_train shape: {X_train_full.shape}\")\n",
        "print(f\"y_train shape: {y_train_full.shape}\")\n",
        "print(f\"X_test shape: {X_test_full.shape}\")\n",
        "\n",
        "# Check for any remaining NaN values\n",
        "print(f\"NaN in X_train: {np.isnan(X_train_full).sum()}\")\n",
        "print(f\"NaN in y_train: {np.isnan(y_train_full).sum()}\")\n",
        "print(f\"NaN in X_test: {np.isnan(X_test_full).sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced scaling with RobustScaler\n",
            "X_train_scaled shape: (28755, 66)\n",
            "X_test_scaled shape: (13148, 66)\n",
            "y_train_scaled shape: (28755,)\n",
            "Scaling completed with RobustScaler!\n"
          ]
        }
      ],
      "source": [
        "# IMPROVED SCALING (RobustScaler is better for time series with outliers)\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "print(\"Enhanced scaling with RobustScaler\")\n",
        "scaler_X = RobustScaler()  # Less sensitive to outliers than StandardScaler\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "# Fit and transform\n",
        "X_train_scaled = scaler_X.fit_transform(X_train_full)\n",
        "X_test_scaled = scaler_X.transform(X_test_full)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train_full.reshape(-1, 1)).flatten()\n",
        "\n",
        "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
        "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
        "print(f\"y_train_scaled shape: {y_train_scaled.shape}\")\n",
        "\n",
        "print(\"Scaling completed with RobustScaler!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using optimized sequence length: 36 hours\n",
            "Optimized sequence shapes:\n",
            "  X_train_seq: (28719, 36, 66)\n",
            "  y_train_seq: (28719,)\n",
            "Samples lost due to sequence creation: 36\n",
            "Remaining training samples: 28719\n"
          ]
        }
      ],
      "source": [
        "# SIMPLIFIED SEQUENCE LENGTH OPTIMIZATION\n",
        "def create_sequences_improved(X, y, sequence_length):\n",
        "    \"\"\"Create sequences with better memory efficiency\"\"\"\n",
        "    X_seq, y_seq = [], []\n",
        "\n",
        "    for i in range(sequence_length, len(X)):\n",
        "        X_seq.append(X[i-sequence_length:i])\n",
        "        y_seq.append(y[i])\n",
        "\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Set optimal sequence length (36 works well based on testing)\n",
        "SEQUENCE_LENGTH = 36\n",
        "print(f\"Using optimized sequence length: {SEQUENCE_LENGTH} hours\")\n",
        "\n",
        "# Create training sequences with optimized length\n",
        "X_train_seq, y_train_seq = create_sequences_improved(X_train_scaled, y_train_scaled, SEQUENCE_LENGTH)\n",
        "\n",
        "print(f\"Optimized sequence shapes:\")\n",
        "print(f\"  X_train_seq: {X_train_seq.shape}\")\n",
        "print(f\"  y_train_seq: {y_train_seq.shape}\")\n",
        "print(f\"Samples lost due to sequence creation: {len(X_train_scaled) - len(X_train_seq)}\")\n",
        "print(f\"Remaining training samples: {len(X_train_seq)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final optimized train-validation split\n",
            "Final training samples: 25272 (maximized)\n",
            "Final validation samples: 3447 (minimized)\n",
            "Training data ratio: 88%\n"
          ]
        }
      ],
      "source": [
        "# FINAL OPTIMIZED SPLIT (More training data)\n",
        "print(\"Final optimized train-validation split\")\n",
        "\n",
        "# Use only 12% for validation (was 15%) = maximum training data\n",
        "val_size = 0.12\n",
        "split_idx = int(len(X_train_seq) * (1 - val_size))\n",
        "\n",
        "X_train_final = X_train_seq[:split_idx]\n",
        "X_val = X_train_seq[split_idx:]\n",
        "y_train_final = y_train_seq[:split_idx]\n",
        "y_val = y_train_seq[split_idx:]\n",
        "\n",
        "print(f\"Final training samples: {len(X_train_final)} (maximized)\")\n",
        "print(f\"Final validation samples: {len(X_val)} (minimized)\")\n",
        "print(f\"Training data ratio: {(1-val_size)*100:.0f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d488782wuR2W"
      },
      "source": [
        "# Model building\n",
        "\n",
        "Build and train LSTM model for time series forecasting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating diverse ensemble...\n",
            "Model 1: LSTM (LR: 0.002)\n",
            "Model 2: GRU (LR: 0.0015)\n",
            "Model 3: Bidirectional (LR: 0.0025)\n",
            "Model 4: LSTM (LR: 0.002)\n"
          ]
        }
      ],
      "source": [
        "# ENSEMBLE OF DIVERSE MODELS\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "def create_diverse_ensemble(input_shape):\n",
        "    \"\"\"Create 4 diverse simple models\"\"\"\n",
        "\n",
        "    # Model 1: Standard LSTM\n",
        "    model1 = Sequential([\n",
        "        LSTM(48, return_sequences=False, input_shape=input_shape),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Model 2: GRU\n",
        "    model2 = Sequential([\n",
        "        GRU(48, return_sequences=False, input_shape=input_shape),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Model 3: Bidirectional LSTM\n",
        "    model3 = Sequential([\n",
        "        Bidirectional(LSTM(24, return_sequences=False, input_shape=input_shape)),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Model 4: Deep narrow\n",
        "    model4 = Sequential([\n",
        "        LSTM(32, return_sequences=True, input_shape=input_shape),\n",
        "        LSTM(24, return_sequences=False),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Different learning rates for diversity\n",
        "    lrs = [0.002, 0.0015, 0.0025, 0.002]\n",
        "    models = [model1, model2, model3, model4]\n",
        "\n",
        "    for i, (model, lr) in enumerate(zip(models, lrs)):\n",
        "        model.compile(optimizer=Adam(learning_rate=lr, clipnorm=1.0),\n",
        "                     loss='mse', metrics=['mae'])\n",
        "        print(f\"Model {i+1}: {model.layers[0].__class__.__name__} (LR: {lr})\")\n",
        "\n",
        "    return models\n",
        "\n",
        "# Create ensemble\n",
        "print(\"Creating diverse ensemble...\")\n",
        "input_shape = (SEQUENCE_LENGTH, X_train_final.shape[2])\n",
        "ensemble_models = create_diverse_ensemble(input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training diverse ensemble...\n",
            "Training model 1/4...\n",
            "Model 1 RMSE: 52.78\n",
            "Training model 2/4...\n",
            "Model 2 RMSE: 53.08\n",
            "Training model 3/4...\n",
            "Model 3 RMSE: 56.00\n",
            "Training model 4/4...\n",
            "Model 4 RMSE: 57.95\n",
            "Individual model RMSEs: ['52.78', '53.08', '56.00', '57.95']\n"
          ]
        }
      ],
      "source": [
        "# TRAIN ENSEMBLE WITH DIFFERENT STRATEGIES\n",
        "print(\"Training diverse ensemble...\")\n",
        "\n",
        "trained_models = []\n",
        "individual_rmses = []\n",
        "\n",
        "for i, model in enumerate(ensemble_models):\n",
        "    print(f\"Training model {i+1}/4...\")\n",
        "\n",
        "    # Different patience for each model\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=20-i*2, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=8-i, min_lr=1e-7)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train_final, y_train_final,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stop, reduce_lr],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate individual model\n",
        "    val_pred_scaled = model.predict(X_val, verbose=0)\n",
        "    val_pred_original = scaler_y.inverse_transform(val_pred_scaled.reshape(-1, 1)).flatten()\n",
        "    val_true_original = scaler_y.inverse_transform(y_val.reshape(-1, 1)).flatten()\n",
        "    rmse = np.sqrt(np.mean((val_true_original - val_pred_original)**2))\n",
        "\n",
        "    print(f\"Model {i+1} RMSE: {rmse:.2f}\")\n",
        "    individual_rmses.append(rmse)\n",
        "    trained_models.append(model)\n",
        "\n",
        "print(f\"Individual model RMSEs: {[f'{r:.2f}' for r in individual_rmses]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating weighted ensemble predictions...\n",
            "Model weights: ['0.260', '0.258', '0.245', '0.237']\n",
            "FINAL ENSEMBLE PERFORMANCE:\n",
            "  Validation RMSE: 51.75\n",
            "  Improvement from 69.67: 17.92 points\n",
            "  Target < 65: ACHIEVED!\n",
            "  Estimated Kaggle score: ~5175\n"
          ]
        }
      ],
      "source": [
        "# WEIGHTED ENSEMBLE PREDICTIONS\n",
        "print(\"Creating weighted ensemble predictions...\")\n",
        "\n",
        "# Calculate performance-based weights\n",
        "inv_rmses = [1/rmse for rmse in individual_rmses]\n",
        "weights = [w/sum(inv_rmses) for w in inv_rmses]\n",
        "print(f\"Model weights: {[f'{w:.3f}' for w in weights]}\")\n",
        "\n",
        "# Validation ensemble\n",
        "val_predictions = []\n",
        "for model in trained_models:\n",
        "    pred_scaled = model.predict(X_val, verbose=0)\n",
        "    pred_original = scaler_y.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
        "    val_predictions.append(pred_original)\n",
        "\n",
        "val_ensemble = np.average(val_predictions, axis=0, weights=weights)\n",
        "val_true_original = scaler_y.inverse_transform(y_val.reshape(-1, 1)).flatten()\n",
        "final_rmse = np.sqrt(np.mean((val_true_original - val_ensemble)**2))\n",
        "\n",
        "print(f\"FINAL ENSEMBLE PERFORMANCE:\")\n",
        "print(f\"  Validation RMSE: {final_rmse:.2f}\")\n",
        "print(f\"  Improvement from 69.67: {69.67 - final_rmse:.2f} points\")\n",
        "print(f\"  Target < 65: {'ACHIEVED!' if final_rmse < 65 else 'Very close!'}\")\n",
        "print(f\"  Estimated Kaggle score: ~{final_rmse * 100:.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating final ensemble test predictions...\n",
            "Creating test sequences...\n",
            "Last training samples: (36, 66)\n",
            "Combined data shape: (13184, 66)\n",
            "Test sequences shape: (13148, 36, 66)\n",
            "Making ensemble predictions...\n",
            "Final test prediction statistics:\n",
            "  Min: 2.5\n",
            "  Max: 385.7\n",
            "  Mean: 95.6\n",
            "FINAL submission saved as: FINAL_submission_RMSE_51_75_ensemble_20250921_153727.csv\n",
            "Expected performance: RMSE 51.75)\n",
            "Target achieved: YES! Below 4000!\n"
          ]
        }
      ],
      "source": [
        "# FINAL ENSEMBLE TEST PREDICTIONS\n",
        "import datetime\n",
        "\n",
        "print(\"Creating final ensemble test predictions...\")\n",
        "\n",
        "# First, create test sequences (this was missing!)\n",
        "print(\"Creating test sequences...\")\n",
        "last_train_X = X_train_scaled[-SEQUENCE_LENGTH:]\n",
        "combined_X = np.vstack([last_train_X, X_test_scaled])\n",
        "\n",
        "print(f\"Last training samples: {last_train_X.shape}\")\n",
        "print(f\"Combined data shape: {combined_X.shape}\")\n",
        "\n",
        "# Create test sequences\n",
        "X_test_sequences = []\n",
        "for i in range(SEQUENCE_LENGTH, len(combined_X)):\n",
        "    X_test_sequences.append(combined_X[i-SEQUENCE_LENGTH:i])\n",
        "\n",
        "X_test_sequences = np.array(X_test_sequences)\n",
        "print(f\"Test sequences shape: {X_test_sequences.shape}\")\n",
        "\n",
        "# Now make predictions from each model\n",
        "print(\"Making ensemble predictions...\")\n",
        "test_predictions = []\n",
        "for model in trained_models:\n",
        "    pred_scaled = model.predict(X_test_sequences, verbose=0)\n",
        "    pred_original = scaler_y.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
        "    test_predictions.append(pred_original)\n",
        "\n",
        "# Weighted ensemble for test\n",
        "test_ensemble = np.average(test_predictions, axis=0, weights=weights)\n",
        "test_ensemble = np.maximum(test_ensemble, 0)  # No negative values\n",
        "\n",
        "print(f\"Final test prediction statistics:\")\n",
        "print(f\"  Min: {test_ensemble.min():.1f}\")\n",
        "print(f\"  Max: {test_ensemble.max():.1f}\")\n",
        "print(f\"  Mean: {test_ensemble.mean():.1f}\")\n",
        "\n",
        "# Create submission with detailed naming\n",
        "def format_datetime_no_leading_zero(dt):\n",
        "    return f\"{dt.year}-{dt.month:02d}-{dt.day:02d} {dt.hour}:{dt.minute:02d}:{dt.second:02d}\"\n",
        "\n",
        "row_ids_formatted = [format_datetime_no_leading_zero(dt) for dt in test.index]\n",
        "\n",
        "submission_final = pd.DataFrame({\n",
        "    'row ID': row_ids_formatted,\n",
        "    'pm2.5': test_ensemble.astype(int)\n",
        "})\n",
        "\n",
        "# Generate detailed filename\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "rmse_str = f\"{final_rmse:.2f}\".replace(\".\", \"_\")\n",
        "submission_path = f'FINAL_submission_RMSE_{rmse_str}_ensemble_{timestamp}.csv'\n",
        "\n",
        "submission_final.to_csv(submission_path, index=False)\n",
        "\n",
        "print(f\"FINAL submission saved as: {submission_path}\")\n",
        "print(f\"Expected performance: RMSE {final_rmse:.2f})\")\n",
        "print(f\"Target achieved: {'YES! Below 4000!' if final_rmse < 67.0 else 'Very close to target!'}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
