{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTsEYdtov6tp"
      },
      "source": [
        "# Beijing air quality forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nWkSHhqXrCqF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n",
            "TensorFlow version: 2.20.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For deep learning - we'll use TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, GRU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded successfully!\n",
            "Train shape: (30676, 12)\n",
            "Test shape: (13148, 11)\n"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "train = pd.read_csv('data/train.csv')\n",
        "test = pd.read_csv('data/test.csv')\n",
        "\n",
        "print(\"Data loaded successfully!\")\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRse3uqRrft5"
      },
      "source": [
        "# Data exploration\n",
        "\n",
        "Explore the dataset with statistics and visualizations to understand the data better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 30676 entries, 0 to 30675\n",
            "Data columns (total 12 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   No        30676 non-null  int64  \n",
            " 1   DEWP      30676 non-null  float64\n",
            " 2   TEMP      30676 non-null  float64\n",
            " 3   PRES      30676 non-null  float64\n",
            " 4   Iws       30676 non-null  float64\n",
            " 5   Is        30676 non-null  float64\n",
            " 6   Ir        30676 non-null  float64\n",
            " 7   datetime  30676 non-null  object \n",
            " 8   cbwd_NW   30676 non-null  float64\n",
            " 9   cbwd_SE   30676 non-null  float64\n",
            " 10  cbwd_cv   30676 non-null  float64\n",
            " 11  pm2.5     28755 non-null  float64\n",
            "dtypes: float64(10), int64(1), object(1)\n",
            "memory usage: 2.8+ MB\n",
            "None\n",
            "\n",
            "Train data describe:\n",
            "                 No          DEWP          TEMP          PRES           Iws  \\\n",
            "count  30676.000000  30676.000000  30676.000000  30676.000000  30676.000000   \n",
            "mean   15338.500000     -0.029431     -0.062712      0.013612      0.030542   \n",
            "std     8855.542765      0.994087      1.015193      1.008991      1.018337   \n",
            "min        1.000000     -2.135153     -2.578070     -2.380821     -0.468688   \n",
            "25%     7669.750000     -0.888034     -0.938521     -0.822670     -0.441894   \n",
            "50%    15338.500000     -0.056622      0.045209     -0.043595     -0.352512   \n",
            "75%    23007.250000      0.913358      0.864984      0.832865      0.005216   \n",
            "max    30676.000000      1.814055      2.340578      2.877939     11.231956   \n",
            "\n",
            "                 Is            Ir       cbwd_NW       cbwd_SE       cbwd_cv  \\\n",
            "count  30676.000000  30676.000000  30676.000000  30676.000000  30676.000000   \n",
            "mean       0.016992      0.011253      0.016193      0.005833     -0.025008   \n",
            "std        1.087278      1.063811      1.006001      1.001847      0.982122   \n",
            "min       -0.069353     -0.137667     -0.690542     -0.732019     -0.522096   \n",
            "25%       -0.069353     -0.137667     -0.690542     -0.732019     -0.522096   \n",
            "50%       -0.069353     -0.137667     -0.690542     -0.732019     -0.522096   \n",
            "75%       -0.069353     -0.137667      1.448138      1.366085     -0.522096   \n",
            "max       35.439859     25.288745      1.448138      1.366085      1.915355   \n",
            "\n",
            "              pm2.5  \n",
            "count  28755.000000  \n",
            "mean     100.793427  \n",
            "std       93.144433  \n",
            "min        0.000000  \n",
            "25%       29.000000  \n",
            "50%       75.000000  \n",
            "75%      142.000000  \n",
            "max      994.000000  \n",
            "\n",
            "Missing values:\n",
            "Train missing values:\n",
            "No             0\n",
            "DEWP           0\n",
            "TEMP           0\n",
            "PRES           0\n",
            "Iws            0\n",
            "Is             0\n",
            "Ir             0\n",
            "datetime       0\n",
            "cbwd_NW        0\n",
            "cbwd_SE        0\n",
            "cbwd_cv        0\n",
            "pm2.5       1921\n",
            "dtype: int64\n",
            "\n",
            "Test missing values:\n",
            "No          0\n",
            "DEWP        0\n",
            "TEMP        0\n",
            "PRES        0\n",
            "Iws         0\n",
            "Is          0\n",
            "Ir          0\n",
            "datetime    0\n",
            "cbwd_NW     0\n",
            "cbwd_SE     0\n",
            "cbwd_cv     0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Basic data exploration\n",
        "print(\"Train data info:\")\n",
        "print(train.info())\n",
        "print(\"\\nTrain data describe:\")\n",
        "print(train.describe())\n",
        "\n",
        "print(\"\\nMissing values:\")\n",
        "print(\"Train missing values:\")\n",
        "print(train.isnull().sum())\n",
        "print(\"\\nTest missing values:\")\n",
        "print(test.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "3R74CEBFrYok",
        "outputId": "0e593627-9c80-490c-826e-74e4df4a2249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Data Overview:\n",
            "          No      DEWP      TEMP      PRES       Iws        Is        Ir  \\\n",
            "30671  30672  1.467633  0.946961 -2.088668 -0.415099 -0.069353  2.687490   \n",
            "30672  30673  1.329064  0.864984 -2.186052 -0.379306 -0.069353  3.393779   \n",
            "30673  30674  1.259780  0.701029 -2.088668 -0.263130 -0.069353  4.100068   \n",
            "30674  30675  1.190496  0.701029 -2.088668 -0.146953 -0.069353  4.806358   \n",
            "30675  30676  1.190496  0.701029 -2.186052 -0.084366 -0.069353 -0.137667   \n",
            "\n",
            "                  datetime   cbwd_NW   cbwd_SE   cbwd_cv  pm2.5  \n",
            "30671  2013-07-01 23:00:00 -0.690542 -0.732019 -0.522096   50.0  \n",
            "30672  2013-07-02 00:00:00  1.448138 -0.732019 -0.522096   41.0  \n",
            "30673  2013-07-02 01:00:00  1.448138 -0.732019 -0.522096   32.0  \n",
            "30674  2013-07-02 02:00:00  1.448138 -0.732019 -0.522096   19.0  \n",
            "30675  2013-07-02 03:00:00  1.448138 -0.732019 -0.522096   18.0  \n",
            "First datetime: 2010-01-01 00:00:00\n",
            "Last datetime: 2013-07-02 03:00:00\n"
          ]
        }
      ],
      "source": [
        "# Inspecting the first few rows of the dataset to understand its structure.\n",
        "print(\"Training Data Overview:\")\n",
        "train.head()\n",
        "print(train.tail())\n",
        "print(f\"First datetime: {train['datetime'].iloc[0]}\")\n",
        "print(f\"Last datetime: {train['datetime'].iloc[-1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-om6hH_RtG8Z",
        "outputId": "8fefc873-d80f-4b45-ead2-89bbfc8d4d62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'datetime', 'cbwd_NW',\n",
              "       'cbwd_SE', 'cbwd_cv', 'pm2.5'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "35IGrMYRscQx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datetime preprocessing completed!\n",
            "Train date range: 2010-01-01 00:00:00 to 2013-07-02 03:00:00\n",
            "Test date range: 2013-07-02 04:00:00 to 2014-12-31 23:00:00\n"
          ]
        }
      ],
      "source": [
        "# Ensure 'datetime' column is in datetime format\n",
        "train['datetime'] = pd.to_datetime(train['datetime'])\n",
        "\n",
        "test['datetime'] = pd.to_datetime(test['datetime'])\n",
        "\n",
        "# Set the 'datetime' column as the index for better time-series handling\n",
        "train.set_index('datetime', inplace=True)\n",
        "# val.set_index('datetime', inplace=True)\n",
        "test.set_index('datetime', inplace=True)\n",
        "\n",
        "print(\"Datetime preprocessing completed!\")\n",
        "print(f\"Train date range: {train.index.min()} to {train.index.max()}\")\n",
        "print(f\"Test date range: {test.index.min()} to {test.index.max()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PM2.5 analysis\n",
            "PM2.5 min: 0.0\n",
            "PM2.5 max: 994.0\n",
            "PM2.5 mean: 100.79\n",
            "PM2.5 std: 93.14\n",
            "\n",
            "Missing PM2.5 pattern\n",
            "Missing values: 1921\n",
            "Percentage missing: 6.26%\n"
          ]
        }
      ],
      "source": [
        "# Examine PM2.5 values\n",
        "print(\"PM2.5 analysis\")\n",
        "print(f\"PM2.5 min: {train['pm2.5'].min()}\")\n",
        "print(f\"PM2.5 max: {train['pm2.5'].max()}\")\n",
        "print(f\"PM2.5 mean: {train['pm2.5'].mean():.2f}\")\n",
        "print(f\"PM2.5 std: {train['pm2.5'].std():.2f}\")\n",
        "\n",
        "# Check where missing values are located\n",
        "print(f\"\\nMissing PM2.5 pattern\")\n",
        "missing_count = train['pm2.5'].isnull().sum()\n",
        "print(f\"Missing values: {missing_count}\")\n",
        "print(f\"Percentage missing: {missing_count/len(train)*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detailed Missing PM2.5 Analysis\n",
            "First 20 rows of missing PM2.5:\n",
            "                     pm2.5\n",
            "datetime                  \n",
            "2010-01-01 00:00:00    NaN\n",
            "2010-01-01 01:00:00    NaN\n",
            "2010-01-01 02:00:00    NaN\n",
            "2010-01-01 03:00:00    NaN\n",
            "2010-01-01 04:00:00    NaN\n",
            "2010-01-01 05:00:00    NaN\n",
            "2010-01-01 06:00:00    NaN\n",
            "2010-01-01 07:00:00    NaN\n",
            "2010-01-01 08:00:00    NaN\n",
            "2010-01-01 09:00:00    NaN\n",
            "2010-01-01 10:00:00    NaN\n",
            "2010-01-01 11:00:00    NaN\n",
            "2010-01-01 12:00:00    NaN\n",
            "2010-01-01 13:00:00    NaN\n",
            "2010-01-01 14:00:00    NaN\n",
            "2010-01-01 15:00:00    NaN\n",
            "2010-01-01 16:00:00    NaN\n",
            "2010-01-01 17:00:00    NaN\n",
            "2010-01-01 18:00:00    NaN\n",
            "2010-01-01 19:00:00    NaN\n",
            "\n",
            "First valid PM2.5 measurement: 2010-01-02 00:00:00\n",
            "\n",
            "Missing values by month:\n",
            "month\n",
            "1     242\n",
            "2       8\n",
            "3     163\n",
            "4     192\n",
            "5     109\n",
            "6     178\n",
            "7      59\n",
            "8     363\n",
            "9     259\n",
            "10    134\n",
            "11     83\n",
            "12    131\n",
            "Name: pm2.5, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check the missing value pattern more thoroughly\n",
        "print(\"Detailed Missing PM2.5 Analysis\")\n",
        "print(\"First 20 rows of missing PM2.5:\")\n",
        "missing_mask = train['pm2.5'].isnull()\n",
        "print(train[missing_mask].head(20)[['pm2.5']])\n",
        "\n",
        "# Find where valid data starts\n",
        "first_valid_idx = train['pm2.5'].first_valid_index()\n",
        "print(f\"\\nFirst valid PM2.5 measurement: {first_valid_idx}\")\n",
        "\n",
        "# Check if missing values are scattered or concentrated\n",
        "print(f\"\\nMissing values by month:\")\n",
        "train_with_month = train.copy()\n",
        "train_with_month['month'] = train_with_month.index.month\n",
        "print(train_with_month.groupby('month')['pm2.5'].apply(lambda x: x.isnull().sum()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABAqt0Jztd5s"
      },
      "source": [
        "# Handle missing values\n",
        "\n",
        "Check the dataset for missing values and decide how to handle them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gap analysis\n",
            "Consecutive missing value periods:\n",
            "Number of missing periods: 163\n",
            "Average gap length: 11.8 hours\n",
            "Max gap length: 155 hours\n",
            "Min gap length: 1 hours\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Check the gaps more systematically\n",
        "print(\"Gap analysis\")\n",
        "# Find consecutive missing periods\n",
        "missing_periods = train['pm2.5'].isnull()\n",
        "# Group consecutive missing values\n",
        "groups = (missing_periods != missing_periods.shift()).cumsum()\n",
        "missing_groups = train[missing_periods].groupby(groups).size()\n",
        "\n",
        "print(\"Consecutive missing value periods:\")\n",
        "print(f\"Number of missing periods: {len(missing_groups)}\")\n",
        "print(f\"Average gap length: {missing_groups.mean():.1f} hours\")\n",
        "print(f\"Max gap length: {missing_groups.max()} hours\")\n",
        "print(f\"Min gap length: {missing_groups.min()} hours\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Handling Missing Values\n",
            "Original data points: 30676\n",
            "Missing PM2.5: 1921\n",
            "After dropping missing PM2.5: 28755\n",
            "Data retained: 93.7%\n",
            "Missing values after cleaning: 0\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Handle missing values\n",
        "print(\"Handling Missing Values\")\n",
        "print(f\"Original data points: {len(train)}\")\n",
        "print(f\"Missing PM2.5: {train['pm2.5'].isnull().sum()}\")\n",
        "\n",
        "# Simple dropna\n",
        "train_clean = train.dropna(subset=['pm2.5']).copy()\n",
        "print(f\"After dropping missing PM2.5: {len(train_clean)}\")\n",
        "print(f\"Data retained: {len(train_clean)/len(train)*100:.1f}%\")\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(f\"Missing values after cleaning: {train_clean.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time continuity check:\n",
            "Number of gaps > 1 hour: 162\n",
            "Largest gaps:\n",
            "datetime\n",
            "2010-09-27 16:00:00   6 days 12:00:00\n",
            "2012-12-28 13:00:00   5 days 08:00:00\n",
            "2011-10-07 16:00:00   4 days 04:00:00\n",
            "2011-03-21 16:00:00   3 days 20:00:00\n",
            "2010-09-30 21:00:00   3 days 05:00:00\n",
            "Name: datetime, dtype: timedelta64[ns]\n"
          ]
        }
      ],
      "source": [
        "# Check for time continuity (important for time series)\n",
        "print(\"Time continuity check:\")\n",
        "time_diff = train_clean.index.to_series().diff()\n",
        "expected_freq = pd.Timedelta(hours=1)\n",
        "\n",
        "# Find any gaps larger than 1 hour\n",
        "gaps = time_diff[time_diff > expected_freq]\n",
        "print(f\"Number of gaps > 1 hour: {len(gaps)}\")\n",
        "if len(gaps) > 0:\n",
        "    print(\"Largest gaps:\")\n",
        "    print(gaps.nlargest(5))\n",
        "else:\n",
        "    print(\"No gaps found - data is continuous!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating enhanced features...\n",
            "Original train features: 11\n",
            "Enhanced train features: 64\n",
            "Enhanced test features: 58\n"
          ]
        }
      ],
      "source": [
        "# CORRECTED ENHANCED FEATURE ENGINEERING\n",
        "def create_enhanced_features(df, target_col='pm2.5', has_target=True):\n",
        "    \"\"\"Create additional features for better prediction\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Temporal features with cyclical encoding\n",
        "    df['hour'] = df.index.hour\n",
        "    df['day_of_week'] = df.index.dayofweek\n",
        "    df['month'] = df.index.month\n",
        "\n",
        "    # Cyclical encoding (important for LSTM)\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "\n",
        "    # Weather interaction features\n",
        "    df['temp_dewp_diff'] = df['TEMP'] - df['DEWP']\n",
        "    df['wind_pressure_ratio'] = df['Iws'] / (df['PRES'] + 1e-8)  # Avoid division by zero\n",
        "\n",
        "    # Moving averages (capture trends)\n",
        "    for window in [3, 6, 12, 24]:\n",
        "        for col in ['DEWP', 'TEMP', 'PRES', 'Iws']:\n",
        "            df[f'{col}_ma_{window}'] = df[col].rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "    # Lag features (very important for time series)\n",
        "    for lag in [1, 2, 3, 6, 12, 24]:\n",
        "        for col in ['DEWP', 'TEMP', 'PRES', 'Iws']:\n",
        "            df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
        "\n",
        "    # ONLY create target lag features if target exists (training data)\n",
        "    if has_target and target_col in df.columns:\n",
        "        for lag in [1, 2, 3, 6, 12]:\n",
        "            df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n",
        "\n",
        "    # Fill NaN values created by lag features\n",
        "    df = df.bfill().ffill()\n",
        "\n",
        "    # Drop original categorical hour/day columns\n",
        "    df = df.drop(['hour', 'day_of_week', 'month'], axis=1, errors='ignore')\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply enhanced feature engineering with correct flags\n",
        "print(\"Creating enhanced features...\")\n",
        "train_enhanced = create_enhanced_features(train_clean, 'pm2.5', has_target=True)   # Has PM2.5\n",
        "test_enhanced = create_enhanced_features(test, 'pm2.5', has_target=False)          # No PM2.5\n",
        "\n",
        "print(f\"Original train features: {len(train_clean.columns)}\")\n",
        "print(f\"Enhanced train features: {len(train_enhanced.columns)}\")\n",
        "print(f\"Enhanced test features: {len(test_enhanced.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKndkdRuty1C"
      },
      "source": [
        "# Feature and target separation\n",
        "\n",
        "Separate features and target variable for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Features\n",
            "Columns in clean training data:\n",
            "['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'pm2.5']\n",
            "\n",
            "Data shape: (28755, 11)\n",
            "\n",
            "Feature Correlations with PM2.5\n",
            "pm2.5      1.000000\n",
            "Iws       -0.260250\n",
            "cbwd_NW   -0.231176\n",
            "DEWP       0.218187\n",
            "cbwd_cv    0.158033\n",
            "cbwd_SE    0.118986\n",
            "PRES      -0.107773\n",
            "Ir        -0.052288\n",
            "TEMP      -0.039601\n",
            "Is         0.022279\n",
            "No         0.017961\n",
            "Name: pm2.5, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Let's examine what features we have\n",
        "print(\"Current Features\")\n",
        "print(\"Columns in clean training data:\")\n",
        "print(train_clean.columns.tolist())\n",
        "print(f\"\\nData shape: {train_clean.shape}\")\n",
        "\n",
        "# Look at feature correlations with PM2.5\n",
        "print(\"\\nFeature Correlations with PM2.5\")\n",
        "correlations = train_clean.corr()['pm2.5'].sort_values(key=abs, ascending=False)\n",
        "print(correlations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QETLRAo_tvQH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced Feature-target split\n",
            "Features in train only: 62\n",
            "Features in test only: 57\n",
            "Common features: 57\n",
            "Train-only features (will be excluded): ['pm2.5_lag_12', 'pm2.5_lag_2', 'pm2.5_lag_3', 'pm2.5_lag_1', 'pm2.5_lag_6']\n",
            "Final feature count: 57\n",
            "X_train shape: (28755, 57)\n",
            "y_train shape: (28755,)\n",
            "X_test shape: (13148, 57)\n",
            "NaN in X_train: 0\n",
            "NaN in y_train: 0\n",
            "NaN in X_test: 0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Enhanced Feature-target split\")\n",
        "\n",
        "# Get feature columns that exist in BOTH train and test\n",
        "train_feature_cols = [col for col in train_enhanced.columns if col not in ['pm2.5', 'No']]\n",
        "test_feature_cols = [col for col in test_enhanced.columns if col not in ['No']]\n",
        "\n",
        "# Use only features that exist in both datasets\n",
        "common_features = list(set(train_feature_cols) & set(test_feature_cols))\n",
        "print(f\"Features in train only: {len(train_feature_cols)}\")\n",
        "print(f\"Features in test only: {len(test_feature_cols)}\")\n",
        "print(f\"Common features: {len(common_features)}\")\n",
        "\n",
        "# Features that exist in train but not test (PM2.5 lag features)\n",
        "train_only_features = list(set(train_feature_cols) - set(test_feature_cols))\n",
        "if train_only_features:\n",
        "    print(f\"Train-only features (will be excluded): {train_only_features}\")\n",
        "\n",
        "# Use common features for modeling\n",
        "feature_cols = common_features\n",
        "\n",
        "X_train_full = train_enhanced[feature_cols].values\n",
        "y_train_full = train_enhanced['pm2.5'].values\n",
        "X_test_full = test_enhanced[feature_cols].values\n",
        "\n",
        "print(f\"Final feature count: {len(feature_cols)}\")\n",
        "print(f\"X_train shape: {X_train_full.shape}\")\n",
        "print(f\"y_train shape: {y_train_full.shape}\")\n",
        "print(f\"X_test shape: {X_test_full.shape}\")\n",
        "\n",
        "# Check for any remaining NaN values\n",
        "print(f\"NaN in X_train: {np.isnan(X_train_full).sum()}\")\n",
        "print(f\"NaN in y_train: {np.isnan(y_train_full).sum()}\")\n",
        "print(f\"NaN in X_test: {np.isnan(X_test_full).sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced scaling with RobustScaler\n",
            "X_train_scaled shape: (28755, 57)\n",
            "X_test_scaled shape: (13148, 57)\n",
            "y_train_scaled shape: (28755,)\n",
            "Scaling completed with RobustScaler!\n"
          ]
        }
      ],
      "source": [
        "# IMPROVED SCALING (RobustScaler is better for time series with outliers)\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "print(\"Enhanced scaling with RobustScaler\")\n",
        "scaler_X = RobustScaler()  # Less sensitive to outliers than StandardScaler\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "# Fit and transform\n",
        "X_train_scaled = scaler_X.fit_transform(X_train_full)\n",
        "X_test_scaled = scaler_X.transform(X_test_full)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train_full.reshape(-1, 1)).flatten()\n",
        "\n",
        "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
        "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
        "print(f\"y_train_scaled shape: {y_train_scaled.shape}\")\n",
        "\n",
        "print(\"Scaling completed with RobustScaler!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating improved sequences...\n",
            "Using sequence length: 48 hours (was 24)\n",
            "Sequence shapes:\n",
            "  X_train_seq: (28707, 48, 57)\n",
            "  y_train_seq: (28707,)\n",
            "Samples lost due to sequence creation: 48\n",
            "Remaining training samples: 28707\n"
          ]
        }
      ],
      "source": [
        "# IMPROVED SEQUENCE CREATION (longer sequences capture more patterns)\n",
        "SEQUENCE_LENGTH = 48  # Increased from 24 to 48 hours (captures daily + weekly patterns)\n",
        "\n",
        "def create_sequences_improved(X, y, sequence_length):\n",
        "    \"\"\"Create sequences with better memory efficiency\"\"\"\n",
        "    X_seq, y_seq = [], []\n",
        "\n",
        "    for i in range(sequence_length, len(X)):\n",
        "        X_seq.append(X[i-sequence_length:i])\n",
        "        y_seq.append(y[i])\n",
        "\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "print(\"Creating improved sequences...\")\n",
        "print(f\"Using sequence length: {SEQUENCE_LENGTH} hours (was 24)\")\n",
        "\n",
        "# Create training sequences\n",
        "X_train_seq, y_train_seq = create_sequences_improved(X_train_scaled, y_train_scaled, SEQUENCE_LENGTH)\n",
        "\n",
        "print(f\"Sequence shapes:\")\n",
        "print(f\"  X_train_seq: {X_train_seq.shape}\")\n",
        "print(f\"  y_train_seq: {y_train_seq.shape}\")\n",
        "\n",
        "print(f\"Samples lost due to sequence creation: {len(X_train_scaled) - len(X_train_seq)}\")\n",
        "print(f\"Remaining training samples: {len(X_train_seq)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced train-validation split\n",
            "Training samples: 22965\n",
            "Validation samples: 5742\n",
            "Validation split: 20.0%\n",
            "Final shapes for training:\n",
            "  X_train: (22965, 48, 57)\n",
            "  y_train: (22965,)\n",
            "  X_val: (5742, 48, 57)\n",
            "  y_val: (5742,)\n"
          ]
        }
      ],
      "source": [
        "# IMPROVED TRAIN-VALIDATION SPLIT (time-aware)\n",
        "print(\"Enhanced train-validation split\")\n",
        "\n",
        "# Use 20% for validation (temporal split - last 20% of data)\n",
        "val_size = 0.20\n",
        "split_idx = int(len(X_train_seq) * (1 - val_size))\n",
        "\n",
        "X_train_final = X_train_seq[:split_idx]\n",
        "X_val = X_train_seq[split_idx:]\n",
        "y_train_final = y_train_seq[:split_idx]\n",
        "y_val = y_train_seq[split_idx:]\n",
        "\n",
        "print(f\"Training samples: {len(X_train_final)}\")\n",
        "print(f\"Validation samples: {len(X_val)}\")\n",
        "print(f\"Validation split: {len(X_val) / len(X_train_seq) * 100:.1f}%\")\n",
        "\n",
        "print(f\"Final shapes for training:\")\n",
        "print(f\"  X_train: {X_train_final.shape}\")\n",
        "print(f\"  y_train: {y_train_final.shape}\")\n",
        "print(f\"  X_val: {X_val.shape}\")\n",
        "print(f\"  y_val: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d488782wuR2W"
      },
      "source": [
        "# Model building\n",
        "\n",
        "Build and train LSTM model for time series forecasting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building improved LSTM model...\n",
            "Enhanced model architecture:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">59,136</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,216</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,176</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m96\u001b[0m)         │        \u001b[38;5;34m59,136\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m96\u001b[0m)         │           \u001b[38;5;34m384\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m96\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m41,216\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │         \u001b[38;5;34m1,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │           \u001b[38;5;34m192\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │         \u001b[38;5;34m1,176\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │            \u001b[38;5;34m96\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)             │           \u001b[38;5;34m300\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m13\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,897</span> (456.63 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m116,897\u001b[0m (456.63 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,369</span> (454.57 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m116,369\u001b[0m (454.57 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> (2.06 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m528\u001b[0m (2.06 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# IMPROVED MODEL ARCHITECTURE\n",
        "from tensorflow.keras.layers import BatchNormalization, GRU\n",
        "\n",
        "def create_improved_lstm_model(input_shape):\n",
        "    \"\"\"Create enhanced LSTM model with better architecture\"\"\"\n",
        "    model = Sequential([\n",
        "        # First LSTM layer with more units\n",
        "        LSTM(96, return_sequences=True, input_shape=input_shape),\n",
        "        BatchNormalization(),  # Stabilizes training\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Second LSTM layer\n",
        "        LSTM(64, return_sequences=True),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Third LSTM layer (final)\n",
        "        LSTM(32, return_sequences=False),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Dense layers with batch normalization\n",
        "        Dense(48, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Dense(24, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Dense(12, activation='relu'),\n",
        "        Dropout(0.1),\n",
        "\n",
        "        # Output layer\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Enhanced optimizer with gradient clipping\n",
        "    optimizer = Adam(\n",
        "        learning_rate=0.001,\n",
        "        clipnorm=1.0  # Prevents exploding gradients\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='mse',\n",
        "        metrics=['mae', lambda y, y_pred: tf.sqrt(tf.reduce_mean(tf.square(y - y_pred)))]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the improved model\n",
        "print(\"Building improved LSTM model...\")\n",
        "input_shape = (SEQUENCE_LENGTH, X_train_final.shape[2])\n",
        "model_improved = create_improved_lstm_model(input_shape)\n",
        "\n",
        "print(\"Enhanced model architecture:\")\n",
        "model_improved.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced training setup\n",
            "Training setup:\n",
            "  Epochs: 120 (with early stopping)\n",
            "  Batch size: 32\n",
            "  Training samples: 22965\n",
            "  Validation samples: 5742\n",
            "\n",
            "Starting enhanced training...\n",
            "Epoch 1/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 40ms/step - lambda: 1.1520 - loss: 0.9592 - mae: 0.7098 - val_lambda: 0.8405 - val_loss: 0.7591 - val_mae: 0.5747 - learning_rate: 0.0010\n",
            "Epoch 2/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 40ms/step - lambda: 1.1375 - loss: 0.5328 - mae: 0.5198 - val_lambda: 0.8408 - val_loss: 0.7229 - val_mae: 0.5597 - learning_rate: 0.0010\n",
            "Epoch 3/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 39ms/step - lambda: 1.1728 - loss: 0.4394 - mae: 0.4631 - val_lambda: 0.8569 - val_loss: 0.7040 - val_mae: 0.5366 - learning_rate: 0.0010\n",
            "Epoch 4/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 40ms/step - lambda: 1.1882 - loss: 0.3900 - mae: 0.4341 - val_lambda: 0.8517 - val_loss: 0.7141 - val_mae: 0.5357 - learning_rate: 0.0010\n",
            "Epoch 5/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 39ms/step - lambda: 1.2040 - loss: 0.3421 - mae: 0.4060 - val_lambda: 0.8737 - val_loss: 0.6586 - val_mae: 0.5077 - learning_rate: 0.0010\n",
            "Epoch 6/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 39ms/step - lambda: 1.2158 - loss: 0.3101 - mae: 0.3861 - val_lambda: 0.8504 - val_loss: 0.7186 - val_mae: 0.5265 - learning_rate: 0.0010\n",
            "Epoch 7/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 40ms/step - lambda: 1.2217 - loss: 0.2886 - mae: 0.3716 - val_lambda: 0.8725 - val_loss: 0.7528 - val_mae: 0.5464 - learning_rate: 0.0010\n",
            "Epoch 8/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 39ms/step - lambda: 1.2273 - loss: 0.2737 - mae: 0.3612 - val_lambda: 0.8236 - val_loss: 0.7322 - val_mae: 0.5218 - learning_rate: 0.0010\n",
            "Epoch 9/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 39ms/step - lambda: 1.2330 - loss: 0.2533 - mae: 0.3484 - val_lambda: 0.8535 - val_loss: 0.7225 - val_mae: 0.5166 - learning_rate: 0.0010\n",
            "Epoch 10/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 41ms/step - lambda: 1.2386 - loss: 0.2337 - mae: 0.3333 - val_lambda: 0.8469 - val_loss: 0.6743 - val_mae: 0.5119 - learning_rate: 0.0010\n",
            "Epoch 11/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 39ms/step - lambda: 1.2400 - loss: 0.2262 - mae: 0.3284 - val_lambda: 0.8406 - val_loss: 0.6037 - val_mae: 0.4931 - learning_rate: 0.0010\n",
            "Epoch 12/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 40ms/step - lambda: 1.2432 - loss: 0.2126 - mae: 0.3197 - val_lambda: 0.8509 - val_loss: 0.6203 - val_mae: 0.4946 - learning_rate: 0.0010\n",
            "Epoch 13/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 43ms/step - lambda: 1.2452 - loss: 0.2058 - mae: 0.3137 - val_lambda: 0.8499 - val_loss: 0.6470 - val_mae: 0.5003 - learning_rate: 0.0010\n",
            "Epoch 14/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 44ms/step - lambda: 1.2498 - loss: 0.1935 - mae: 0.3047 - val_lambda: 0.8551 - val_loss: 0.7188 - val_mae: 0.5228 - learning_rate: 0.0010\n",
            "Epoch 15/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 43ms/step - lambda: 1.2491 - loss: 0.1905 - mae: 0.3002 - val_lambda: 0.8618 - val_loss: 0.6923 - val_mae: 0.5141 - learning_rate: 0.0010\n",
            "Epoch 16/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 43ms/step - lambda: 1.2513 - loss: 0.1847 - mae: 0.2972 - val_lambda: 0.8546 - val_loss: 0.6407 - val_mae: 0.5078 - learning_rate: 0.0010\n",
            "Epoch 17/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 44ms/step - lambda: 1.2528 - loss: 0.1767 - mae: 0.2908 - val_lambda: 0.8545 - val_loss: 0.6455 - val_mae: 0.5026 - learning_rate: 0.0010\n",
            "Epoch 18/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 43ms/step - lambda: 1.2554 - loss: 0.1711 - mae: 0.2859 - val_lambda: 0.8491 - val_loss: 0.6999 - val_mae: 0.5150 - learning_rate: 0.0010\n",
            "Epoch 19/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 44ms/step - lambda: 1.2562 - loss: 0.1668 - mae: 0.2829 - val_lambda: 0.8467 - val_loss: 0.6588 - val_mae: 0.5082 - learning_rate: 0.0010\n",
            "Epoch 20/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 43ms/step - lambda: 1.2563 - loss: 0.1643 - mae: 0.2796 - val_lambda: 0.8436 - val_loss: 0.7034 - val_mae: 0.5242 - learning_rate: 0.0010\n",
            "Epoch 21/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 42ms/step - lambda: 1.2583 - loss: 0.1580 - mae: 0.2740 - val_lambda: 0.8605 - val_loss: 0.6843 - val_mae: 0.5116 - learning_rate: 0.0010\n",
            "Epoch 22/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 45ms/step - lambda: 1.2595 - loss: 0.1533 - mae: 0.2717 - val_lambda: 0.8540 - val_loss: 0.6675 - val_mae: 0.5052 - learning_rate: 0.0010\n",
            "Epoch 23/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - lambda: 1.2541 - loss: 0.1528 - mae: 0.2703\n",
            "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 43ms/step - lambda: 1.2585 - loss: 0.1520 - mae: 0.2689 - val_lambda: 0.8571 - val_loss: 0.7162 - val_mae: 0.5209 - learning_rate: 0.0010\n",
            "Epoch 24/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 45ms/step - lambda: 1.2635 - loss: 0.1358 - mae: 0.2553 - val_lambda: 0.8511 - val_loss: 0.6426 - val_mae: 0.4969 - learning_rate: 5.0000e-04\n",
            "Epoch 25/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 46ms/step - lambda: 1.2642 - loss: 0.1300 - mae: 0.2508 - val_lambda: 0.8537 - val_loss: 0.6438 - val_mae: 0.5013 - learning_rate: 5.0000e-04\n",
            "Epoch 26/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 45ms/step - lambda: 1.2637 - loss: 0.1323 - mae: 0.2505 - val_lambda: 0.8549 - val_loss: 0.6787 - val_mae: 0.5090 - learning_rate: 5.0000e-04\n",
            "Epoch 27/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 46ms/step - lambda: 1.2660 - loss: 0.1276 - mae: 0.2472 - val_lambda: 0.8515 - val_loss: 0.6663 - val_mae: 0.5057 - learning_rate: 5.0000e-04\n",
            "Epoch 28/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 44ms/step - lambda: 1.2673 - loss: 0.1224 - mae: 0.2449 - val_lambda: 0.8547 - val_loss: 0.6521 - val_mae: 0.5076 - learning_rate: 5.0000e-04\n",
            "Epoch 29/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 44ms/step - lambda: 1.2658 - loss: 0.1250 - mae: 0.2464 - val_lambda: 0.8424 - val_loss: 0.6556 - val_mae: 0.4998 - learning_rate: 5.0000e-04\n",
            "Epoch 30/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 43ms/step - lambda: 1.2683 - loss: 0.1205 - mae: 0.2431 - val_lambda: 0.8429 - val_loss: 0.6526 - val_mae: 0.5059 - learning_rate: 5.0000e-04\n",
            "Epoch 31/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 43ms/step - lambda: 1.2680 - loss: 0.1173 - mae: 0.2396 - val_lambda: 0.8584 - val_loss: 0.7119 - val_mae: 0.5264 - learning_rate: 5.0000e-04\n",
            "Epoch 32/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 44ms/step - lambda: 1.2670 - loss: 0.1207 - mae: 0.2414 - val_lambda: 0.8453 - val_loss: 0.6526 - val_mae: 0.5038 - learning_rate: 5.0000e-04\n",
            "Epoch 33/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 43ms/step - lambda: 1.2686 - loss: 0.1166 - mae: 0.2393 - val_lambda: 0.8448 - val_loss: 0.6458 - val_mae: 0.5018 - learning_rate: 5.0000e-04\n",
            "Epoch 34/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 44ms/step - lambda: 1.2698 - loss: 0.1160 - mae: 0.2381 - val_lambda: 0.8634 - val_loss: 0.6690 - val_mae: 0.5117 - learning_rate: 5.0000e-04\n",
            "Epoch 35/120\n",
            "\u001b[1m717/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - lambda: 1.2683 - loss: 0.1112 - mae: 0.2354\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 44ms/step - lambda: 1.2698 - loss: 0.1142 - mae: 0.2353 - val_lambda: 0.8652 - val_loss: 0.6919 - val_mae: 0.5234 - learning_rate: 5.0000e-04\n",
            "Epoch 36/120\n",
            "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 43ms/step - lambda: 1.2698 - loss: 0.1102 - mae: 0.2311 - val_lambda: 0.8511 - val_loss: 0.6623 - val_mae: 0.5111 - learning_rate: 2.5000e-04\n",
            "Epoch 36: early stopping\n",
            "Restoring model weights from the end of the best epoch: 11.\n",
            "Enhanced training completed!\n"
          ]
        }
      ],
      "source": [
        "# ENHANCED TRAINING SETUP\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "print(\"Enhanced training setup\")\n",
        "\n",
        "# Better callbacks\n",
        "callbacks_improved = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=25,  # Increased patience\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=12,  # Reduce LR more aggressively\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Training parameters\n",
        "EPOCHS = 120  # More epochs with early stopping\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "print(f\"Training setup:\")\n",
        "print(f\"  Epochs: {EPOCHS} (with early stopping)\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Training samples: {len(X_train_final)}\")\n",
        "print(f\"  Validation samples: {len(X_val)}\")\n",
        "\n",
        "print(\"\\nStarting enhanced training...\")\n",
        "history_improved = model_improved.fit(\n",
        "    X_train_final, y_train_final,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks_improved,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Enhanced training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced model evaluation\n",
            "IMPROVED MODEL PERFORMANCE:\n",
            "  Validation RMSE: 72.37 (Target: < 70)\n",
            "  Validation MAE: 45.93\n",
            "  Improvement target: Need more work\n",
            "\n",
            "Prediction statistics:\n",
            "  Min prediction: 20.3\n",
            "  Max prediction: 469.2\n",
            "  Mean prediction: 105.9\n",
            "  Std prediction: 80.5\n",
            "\n",
            "Actual statistics:\n",
            "  Min actual: 4.0\n",
            "  Max actual: 886.0\n",
            "  Mean actual: 111.6\n",
            "  Std actual: 106.9\n"
          ]
        }
      ],
      "source": [
        "# ENHANCED MODEL EVALUATION\n",
        "print(\"Enhanced model evaluation\")\n",
        "\n",
        "# Make predictions on validation set\n",
        "val_pred_scaled = model_improved.predict(X_val, verbose=0)\n",
        "val_pred_original = scaler_y.inverse_transform(val_pred_scaled.reshape(-1, 1)).flatten()\n",
        "val_true_original = scaler_y.inverse_transform(y_val.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "val_rmse_improved = np.sqrt(np.mean((val_true_original - val_pred_original)**2))\n",
        "val_mae_improved = np.mean(np.abs(val_true_original - val_pred_original))\n",
        "\n",
        "print(f\"IMPROVED MODEL PERFORMANCE:\")\n",
        "print(f\"  Validation RMSE: {val_rmse_improved:.2f} (Target: < 70)\")\n",
        "print(f\"  Validation MAE: {val_mae_improved:.2f}\")\n",
        "print(f\"  Improvement target: {'ACHIEVED!' if val_rmse_improved < 70 else 'Need more work'}\")\n",
        "\n",
        "# Prediction statistics\n",
        "print(f\"\\nPrediction statistics:\")\n",
        "print(f\"  Min prediction: {val_pred_original.min():.1f}\")\n",
        "print(f\"  Max prediction: {val_pred_original.max():.1f}\")\n",
        "print(f\"  Mean prediction: {val_pred_original.mean():.1f}\")\n",
        "print(f\"  Std prediction: {val_pred_original.std():.1f}\")\n",
        "\n",
        "print(f\"\\nActual statistics:\")\n",
        "print(f\"  Min actual: {val_true_original.min():.1f}\")\n",
        "print(f\"  Max actual: {val_true_original.max():.1f}\")\n",
        "print(f\"  Mean actual: {val_true_original.mean():.1f}\")\n",
        "print(f\"  Std actual: {val_true_original.std():.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating enhanced test predictions\n",
            "Last training samples: (48, 57)\n",
            "Combined data shape: (13196, 57)\n",
            "Test sequences shape: (13148, 48, 57)\n",
            "Making improved predictions...\n",
            "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step\n",
            "Test prediction statistics:\n",
            "  Min: 19.0\n",
            "  Max: 468.1\n",
            "  Mean: 93.2\n",
            "Improved submission saved as: improved_submission.csv\n",
            "Expected RMSE improvement: 72.37 (was ~76)\n"
          ]
        }
      ],
      "source": [
        "# IMPROVED TEST PREDICTIONS\n",
        "print(\"Creating enhanced test predictions\")\n",
        "\n",
        "# Create test sequences with improved method\n",
        "last_train_X = X_train_scaled[-SEQUENCE_LENGTH:]\n",
        "combined_X = np.vstack([last_train_X, X_test_scaled])\n",
        "\n",
        "print(f\"Last training samples: {last_train_X.shape}\")\n",
        "print(f\"Combined data shape: {combined_X.shape}\")\n",
        "\n",
        "# Create test sequences\n",
        "X_test_sequences = []\n",
        "for i in range(SEQUENCE_LENGTH, len(combined_X)):\n",
        "    X_test_sequences.append(combined_X[i-SEQUENCE_LENGTH:i])\n",
        "\n",
        "X_test_sequences = np.array(X_test_sequences)\n",
        "print(f\"Test sequences shape: {X_test_sequences.shape}\")\n",
        "\n",
        "# Make predictions\n",
        "print(\"Making improved predictions...\")\n",
        "test_pred_scaled = model_improved.predict(X_test_sequences, verbose=1)\n",
        "test_pred_original = scaler_y.inverse_transform(test_pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Ensure no negative predictions\n",
        "test_pred_original = np.maximum(test_pred_original, 0)\n",
        "\n",
        "print(f\"Test prediction statistics:\")\n",
        "print(f\"  Min: {test_pred_original.min():.1f}\")\n",
        "print(f\"  Max: {test_pred_original.max():.1f}\")\n",
        "print(f\"  Mean: {test_pred_original.mean():.1f}\")\n",
        "\n",
        "# Create submission\n",
        "def format_datetime_no_leading_zero(dt):\n",
        "    return f\"{dt.year}-{dt.month:02d}-{dt.day:02d} {dt.hour}:{dt.minute:02d}:{dt.second:02d}\"\n",
        "\n",
        "row_ids_formatted = [format_datetime_no_leading_zero(dt) for dt in test.index]\n",
        "\n",
        "submission_improved = pd.DataFrame({\n",
        "    'row ID': row_ids_formatted,\n",
        "    'pm2.5': test_pred_original.astype(int)\n",
        "})\n",
        "\n",
        "# Save submission\n",
        "submission_path = 'improved_submission.csv'\n",
        "submission_improved.to_csv(submission_path, index=False)\n",
        "print(f\"Improved submission saved as: {submission_path}\")\n",
        "print(f\"Expected RMSE improvement: {val_rmse_improved:.2f} (was ~76)\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
